- 当前我们才用了 **13k/190k tokens**，还有 **17万+ 的余量**
推荐顺序：
1. 文档处理模块（PDF/Excel）
2. 向量化+存储模块
3. 检索模块
4. LLM问答模块

5. 主流程整合 ？？？


1. 创建一个 main.py 作为总入口
功能：

初始化所有资源（embedding、vectorstore、rag）
提供交互式问答界面
支持文档管理（添加、删除、查看）
支持切换模型

开始优化
1， 目前的回答质量不行，感觉是检索的块太少所致，写死3不是很好的方式，有什么办法优化呢？
2， rerank加入，需要考虑什么问题才使用rerank？
3， 加入结构性文档数据分块
4， embedding 增加一个api的方式，如果要部署到个人服务器，内存太贵，演示使用用api即可；

5， 分块相邻语义相似度曲线，用于观察分块语义效果
6， 制作一个前端，用vue
    1）简单的登录
    2）进入分块页面
        2-1）分块操作页面，上传，分块简单配置，分块方式选择
        2-2）分块结果查看，确认分块
        2-3）分块相邻语义相似度曲线，查看
        2-4）分块召回测试
        2-5）满意了之后才开始正式分块，上诉结果不满意不得提交到正式的 Chroma
        2-6）提供文档名称，进行删除对应文档的所有向量数据（用于同一个文档，重新分块，要不然考虑在正式分块之前直接尝试这一步骤）
    3）进入对话页面，提问，切换模型，





src/
├── tools/
│   ├── __init__.py
│   └── embedding_text_splitter.py    # ✅ 语义分块器
├── processors/
│   ├── __init__.py
│   └── semantic_chunker.py            # ✅ 文本处理器
└── loaders/
    ├── __init__.py
    └── pdf_loader.py                  # ✅ PDF加载器



