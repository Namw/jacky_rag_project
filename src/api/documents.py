from fastapi import UploadFile, File, HTTPException, APIRouter, Depends
from fastapi.responses import JSONResponse
from langchain_chroma import Chroma
from pydantic import BaseModel, Field
from typing import List, Optional
from sentence_transformers import CrossEncoder
import uuid
import os
from pathlib import Path
from datetime import datetime
import fitz  # PyMuPDF
import time
import chromadb
from models.model_paths import get_models_cache_dir
from src.api.auth import get_current_user, User
from src.services.retrieval_service import retrieve_with_rerank, embedding_model, CHROMA_PERMANENT_DIR
from src.services.vector_store_cache import vectorstore_cache

# ä¿®æ”¹routerçš„prefixå’Œtags
router = APIRouter(
    prefix="/api/documents",
    tags=["Documents"],
)

# é…ç½®
UPLOAD_DIR = Path("data/uploads")
UPLOAD_DIR.mkdir(exist_ok=True)
MAX_FILE_SIZE = 5 * 1024 * 1024  # 5MB

# Chromaå­˜å‚¨è·¯å¾„
CHROMA_TEMP_DIR = Path("data/vectorstore/temp")  # ä¸´æ—¶åº“
CHROMA_TEMP_DIR.mkdir(exist_ok=True)
CHROMA_PERMANENT_DIR.mkdir(exist_ok=True)

# ç®€å•çš„å†…å­˜å­˜å‚¨ï¼ˆç”Ÿäº§ç¯å¢ƒåº”è¯¥ç”¨æ•°æ®åº“ï¼‰
documents_db = {}


class Document:
    def __init__(self, document_id: str, filename: str, filepath: str,
                 page_count: int, file_size: int, user_id: str):
        self.document_id = document_id
        self.filename = filename
        self.filepath = filepath
        self.page_count = page_count
        self.file_size = file_size
        self.user_id = user_id
        self.status = "uploaded"
        self.created_at = datetime.now()
        self.text_content = None
        self.chunks = None
        self.chroma_collection_name = None
        self.permanent_collection_name = None  # æ­£å¼åº“collectionåç§°
        self.confirmed_at = None  # ç¡®è®¤æ—¶é—´

try:
    reranker_model = CrossEncoder(
        model_name_or_path = get_models_cache_dir() + '/BAAI-bge-reranker-large',
        max_length=512,
        device='cpu'
    )
    print("âœ… Rerankeræ¨¡å‹åŠ è½½æˆåŠŸ")
except Exception as e:
    print(f"âš ï¸ Rerankeræ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    reranker_model = None

def cleanup_temp_collection(collection_name: str):
    """åˆ é™¤Chromaä¸´æ—¶collection"""
    try:
        client = chromadb.PersistentClient(path=str(CHROMA_TEMP_DIR))
        client.delete_collection(name=collection_name)
        print(f"ğŸ—‘ï¸ åˆ é™¤ä¸´æ—¶collection: {collection_name}")
    except Exception as e:
        print(f"âš ï¸ åˆ é™¤collectionå¤±è´¥ {collection_name}: {e}")


def verify_document_ownership(document_id: str, user_id: str):
    """éªŒè¯æ–‡æ¡£æ‰€æœ‰æƒ"""
    if document_id not in documents_db:
        raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")

    doc = documents_db[document_id]

    if doc.user_id != user_id:
        raise HTTPException(status_code=403, detail="æ— æƒè®¿é—®æ­¤æ–‡æ¡£")

    return doc


# ==================== Pydantic Models ====================

class ChunkRequest(BaseModel):
    """åˆ†å—è¯·æ±‚å‚æ•°"""
    chunk_size: int = Field(default=500, ge=100, le=2000, description="åˆ†å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰")
    overlap: int = Field(default=50, ge=0, le=500, description="é‡å å­—ç¬¦æ•°")
    separator: str = Field(default="\n\n", description="åˆ†éš”ç¬¦")


class ChunkItem(BaseModel):
    """å•ä¸ªåˆ†å—"""
    chunk_id: str
    content: str
    start_pos: int
    end_pos: int
    char_count: int
    index: int


class ChunkResponse(BaseModel):
    """åˆ†å—å“åº”"""
    document_id: str
    chunks: List[ChunkItem]
    total_chunks: int
    total_chars: int
    chunk_size: int
    overlap: int


class VectorizeResponse(BaseModel):
    """å‘é‡åŒ–å“åº”"""
    document_id: str
    status: str
    total_chunks: int
    embedding_dim: int
    message: str


class SearchRequest(BaseModel):
    """æœç´¢è¯·æ±‚å‚æ•°"""
    query: str = Field(..., min_length=1, description="æœç´¢é—®é¢˜")
    top_k: int = Field(default=5, ge=1, le=20, description="è¿”å›top-kä¸ªæœ€ç›¸å…³çš„chunks")
    threshold: Optional[float] = Field(default=None, ge=0.0, le=1.0, description="ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆå¯é€‰ï¼‰")
    use_rerank: bool = Field(default=False, description="æ˜¯å¦å¯ç”¨rerankäºŒæ¬¡ç²¾æ’")  # æ–°å¢ â­ï¸


class SearchResultItem(BaseModel):
    """å•ä¸ªæœç´¢ç»“æœ"""
    chunk_id: str
    chunk_index: int
    content: str
    similarity_score: float
    char_count: int
    start_pos: int
    end_pos: int


class SearchResponse(BaseModel):
    """æœç´¢å“åº”"""
    document_id: str
    query: str
    results: List[SearchResultItem]
    total_results: int
    search_time_ms: float


class ConfirmResponse(BaseModel):
    """ç¡®è®¤å…¥åº“å“åº”"""
    document_id: str
    status: str
    permanent_collection_name: str
    total_chunks: int
    confirmed_at: str
    message: str

class PermanentChunkItem(BaseModel):
    """æ­£å¼åº“ä¸­çš„å•ä¸ªåˆ†å—"""
    chunk_id: str
    chunk_index: int
    content: str
    char_count: int
    start_pos: int
    end_pos: int
    metadata: dict


class PermanentDocumentResponse(BaseModel):
    """æ­£å¼åº“æ–‡æ¡£æŸ¥çœ‹å“åº”"""
    document_id: str
    permanent_collection_name: str
    total_chunks: int
    chunks: List[PermanentChunkItem]
    page: int
    page_size: int
    has_more: bool

# ==================== å·¥å…·å‡½æ•° ====================

def split_text_with_overlap(text: str, chunk_size: int, overlap: int, separator: str) -> List[dict]:
    """æ–‡æœ¬åˆ†å—å‡½æ•°"""
    chunks = []
    text_length = len(text)
    start = 0
    index = 0

    while start < text_length:
        end = start + chunk_size

        if end < text_length:
            search_start = max(start, end - 100)
            search_end = min(text_length, end + 100)
            search_text = text[search_start:search_end]

            sep_pos = search_text.rfind(separator)
            if sep_pos != -1:
                end = search_start + sep_pos + len(separator)
        else:
            end = text_length

        chunk_content = text[start:end].strip()

        if chunk_content:
            chunk_id = f"chunk_{index}"
            chunks.append({
                "chunk_id": chunk_id,
                "content": chunk_content,
                "start_pos": start,
                "end_pos": end,
                "char_count": len(chunk_content),
                "index": index
            })
            index += 1

        start = end - overlap

        if start >= text_length or (end >= text_length and start == end - overlap):
            break

    return chunks

async def rerank_results(query: str, results: List[dict], top_k: int) -> List[dict]:
    """ä½¿ç”¨BGE rerankeræ¨¡å‹è¿›è¡ŒäºŒæ¬¡ç²¾æ’"""

    # æ£€æŸ¥rerankeræ˜¯å¦å¯ç”¨
    if reranker_model is None:
        print("âš ï¸ Rerankerä¸å¯ç”¨ï¼Œè¿”å›åŸå§‹ç»“æœ")
        return results[:top_k]

    try:
        # å‡†å¤‡query-documentå¯¹
        pairs = [[query, item['content']] for item in results]

        # è®¡ç®—rerankåˆ†æ•°
        rerank_scores = reranker_model.predict(pairs)

        # æ›´æ–°ç›¸ä¼¼åº¦åˆ†æ•°
        for i, item in enumerate(results):
            item['similarity_score'] = round(float(rerank_scores[i]), 4)

        # æŒ‰rerankåˆ†æ•°é™åºæ’åº
        results.sort(key=lambda x: x['similarity_score'], reverse=True)

        return results[:top_k]

    except Exception as e:
        print(f"âš ï¸ Rerankå¤±è´¥: {e}")
        return results[:top_k]


def extract_category_from_chunks(chunks: List[dict], max_chunks: int = 3) -> str:
    """
    ä»å‰å‡ ä¸ªchunkæå–æ–‡æ¡£åˆ†ç±»ï¼ˆå…³é”®è¯åŒ¹é…ï¼‰
    """
    # åˆå¹¶å‰å‡ ä¸ªchunkçš„å†…å®¹
    sample_text = " ".join([chunk["content"] for chunk in chunks[:max_chunks]])

    # å…³é”®è¯åŒ¹é…è§„åˆ™ï¼ˆå¯ä»¥æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼‰
    rules = {
        "ç®€å†": ["ç®€å†", "å·¥ä½œç»éªŒ", "æ•™è‚²èƒŒæ™¯", "æ±‚èŒæ„å‘", "ä¸ªäººä¿¡æ¯", "æŠ€èƒ½ç‰¹é•¿"],
        "åŠ³åŠ¨åˆåŒ": ["åŠ³åŠ¨åˆåŒ", "ç”²æ–¹", "ä¹™æ–¹", "åˆåŒç¼–å·", "ç­¾è®¢æ—¥æœŸ", "åˆåŒæœŸé™"],
        "å…¬å¸ç®¡ç†åˆ¶åº¦": ["ç®¡ç†åˆ¶åº¦", "è§„ç« åˆ¶åº¦", "ç¬¬ä¸€ç« ", "æ€»åˆ™", "ç¬¬ä¸€æ¡", "å‘˜å·¥å®ˆåˆ™"],
        "è´¢åŠ¡æŠ¥è¡¨": ["èµ„äº§è´Ÿå€ºè¡¨", "åˆ©æ¶¦è¡¨", "ç°é‡‘æµé‡", "è´¢åŠ¡æŠ¥è¡¨", "ä¼šè®¡æœŸé—´"],
        "ä¼šè®®çºªè¦": ["ä¼šè®®çºªè¦", "å‚ä¼šäººå‘˜", "ä¼šè®®æ—¶é—´", "ä¼šè®®è®®é¢˜", "å†³è®®äº‹é¡¹"]
    }

    # è®¡ç®—æ¯ä¸ªåˆ†ç±»çš„åŒ¹é…åˆ†æ•°
    category_scores = {}
    for category, keywords in rules.items():
        score = sum(1 for keyword in keywords if keyword in sample_text)
        category_scores[category] = score

    # æ‰¾åˆ°å¾—åˆ†æœ€é«˜çš„åˆ†ç±»
    best_category = max(category_scores.items(), key=lambda x: x[1])

    # å¦‚æœå¾—åˆ†ä¸º0ï¼Œè¯´æ˜éƒ½æ²¡åŒ¹é…åˆ°
    if best_category[1] == 0:
        return "å…¶ä»–æ–‡æ¡£"

    return best_category[0]

# ==================== API Endpoints ====================

@router.post("/upload")
async def upload_document(
        file: UploadFile = File(...),
        current_user: User = Depends(get_current_user)
):
    """ä¸Šä¼ PDFæ–‡ä»¶"""

    if not file.filename.endswith('.pdf'):
        raise HTTPException(status_code=400, detail="åªæ”¯æŒPDFæ–‡ä»¶")

    content = await file.read()
    file_size = len(content)

    if file_size > MAX_FILE_SIZE:
        raise HTTPException(
            status_code=400,
            detail=f"æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶ï¼ˆæœ€å¤§5MBï¼‰ï¼Œå½“å‰æ–‡ä»¶: {file_size / 1024 / 1024:.2f}MB"
        )

    try:
        document_id = str(uuid.uuid4())
        filepath = UPLOAD_DIR / f"{document_id}.pdf"

        with open(filepath, "wb") as f:
            f.write(content)

        doc_pdf = fitz.open(filepath)
        page_count = doc_pdf.page_count

        text_content = ""
        for page in doc_pdf:
            text_content += page.get_text() + "\n"

        doc_pdf.close()

        if not text_content.strip():
            os.remove(filepath)
            raise HTTPException(status_code=400, detail="PDFæ–‡ä»¶æ— æ³•æå–æ–‡æœ¬å†…å®¹")

    except Exception as e:
        if filepath.exists():
            os.remove(filepath)
        raise HTTPException(status_code=400, detail=f"PDFæ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}")

    doc = Document(
        document_id=document_id,
        filename=file.filename,
        filepath=str(filepath),
        page_count=page_count,
        file_size=file_size,
        user_id=current_user.username  # ä½¿ç”¨ username
    )
    doc.text_content = text_content

    documents_db[document_id] = doc

    return JSONResponse(
        status_code=200,
        content={
            "document_id": document_id,
            "filename": file.filename,
            "file_size": file_size,
            "page_count": page_count,
            "status": "uploaded",
            "created_at": doc.created_at.isoformat()
        }
    )

@router.post("/{document_id}/chunk", response_model=ChunkResponse)
async def chunk_document(
        document_id: str,
        request: ChunkRequest,
        current_user: User = Depends(get_current_user)
):
    """å¯¹æ–‡æ¡£è¿›è¡Œåˆ†å—ï¼ˆå…è®¸é‡å¤åˆ†å—ï¼Œè‡ªåŠ¨è¦†ç›–æ—§æ•°æ®ï¼‰"""
    doc = verify_document_ownership(document_id, current_user.username)

    if doc.status not in ["uploaded", "chunked"]:
        raise HTTPException(
            status_code=400,
            detail=f"æ–‡æ¡£çŠ¶æ€é”™è¯¯ï¼Œå½“å‰çŠ¶æ€: {doc.status}ï¼Œåªèƒ½å¯¹ uploaded æˆ– chunked çŠ¶æ€çš„æ–‡æ¡£è¿›è¡Œåˆ†å—"
        )

    if not doc.text_content:
        raise HTTPException(status_code=400, detail="æ–‡æ¡£æ²¡æœ‰æ–‡æœ¬å†…å®¹")

    if request.overlap >= request.chunk_size:
        raise HTTPException(status_code=400, detail="overlapä¸èƒ½å¤§äºæˆ–ç­‰äºchunk_size")

    try:
        chunks = split_text_with_overlap(
            text=doc.text_content,
            chunk_size=request.chunk_size,
            overlap=request.overlap,
            separator=request.separator
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ†å—å¤±è´¥: {str(e)}")

    # ğŸ‘‡ æ–°å¢ï¼šè‡ªåŠ¨æå–åˆ†ç±»
    category = extract_category_from_chunks(chunks, max_chunks=3)

    doc.category = category  # ğŸ‘ˆ ä¿å­˜åˆ†ç±»
    doc.chunks = chunks
    doc.status = "chunked"

    return ChunkResponse(
        document_id=document_id,
        chunks=[ChunkItem(**chunk) for chunk in chunks],
        total_chunks=len(chunks),
        total_chars=len(doc.text_content),
        chunk_size=request.chunk_size,
        overlap=request.overlap,
        category=category  # ğŸ‘ˆ è¿”å›ç»™å‰ç«¯
    )


@router.post("/{document_id}/vectorize", response_model=VectorizeResponse)
async def vectorize_document(
        document_id: str,
        current_user: User = Depends(get_current_user)
):
    """å¯¹æ–‡æ¡£åˆ†å—è¿›è¡Œå‘é‡åŒ–ï¼ˆå…è®¸é‡å¤å‘é‡åŒ–ï¼Œè‡ªåŠ¨è¦†ç›–æ—§æ•°æ®ï¼‰"""
    doc = verify_document_ownership(document_id, current_user.username)

    if doc.status not in ["chunked", "vectorized"]:
        raise HTTPException(
            status_code=400,
            detail=f"æ–‡æ¡£çŠ¶æ€é”™è¯¯ï¼Œå½“å‰çŠ¶æ€: {doc.status}ï¼Œå¿…é¡»å…ˆå®Œæˆåˆ†å—"
        )

    if not doc.chunks or len(doc.chunks) == 0:
        raise HTTPException(status_code=400, detail="æ–‡æ¡£æ²¡æœ‰åˆ†å—æ•°æ®")

    # å¦‚æœå·²ç»å‘é‡åŒ–è¿‡ï¼Œå…ˆæ¸…ç†æ—§çš„Chroma collection
    if doc.status == "vectorized" and doc.chroma_collection_name:
        try:
            cleanup_temp_collection(doc.chroma_collection_name)
            print(f"âœ… å·²æ¸…ç†æ—§çš„å‘é‡æ•°æ®: {doc.chroma_collection_name}")
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†æ—§collectionå¤±è´¥ï¼ˆå¿½ç•¥ï¼‰: {e}")

    chunk_texts = [chunk["content"] for chunk in doc.chunks]
    chunk_ids = [chunk["chunk_id"] for chunk in doc.chunks]

    metadatas = [
        {
            "chunk_index": chunk["index"],
            "char_count": chunk["char_count"],
            "start_pos": chunk["start_pos"],
            "end_pos": chunk["end_pos"],
            "document_id": document_id
        }
        for chunk in doc.chunks
    ]

    try:
        collection_name = f"temp_{document_id.replace('-', '_')}"

        Chroma.from_texts(
            texts=chunk_texts,
            embedding=embedding_model,
            ids=chunk_ids,
            metadatas=metadatas,
            collection_name=collection_name,
            persist_directory=str(CHROMA_TEMP_DIR),
            collection_metadata={"hnsw:space": "cosine"}
        )

        doc.chroma_collection_name = collection_name

        sample_embedding = embedding_model.embed_query(chunk_texts[0])
        embedding_dim = len(sample_embedding)

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"å‘é‡åŒ–å¤±è´¥: {str(e)}"
        )

    doc.status = "vectorized"

    return VectorizeResponse(
        document_id=document_id,
        status="vectorized",
        total_chunks=len(doc.chunks),
        embedding_dim=embedding_dim,
        message=f"æˆåŠŸå‘é‡åŒ– {len(doc.chunks)} ä¸ªæ–‡æœ¬å—å¹¶å­˜å…¥Chromaä¸´æ—¶åº“"
    )


@router.post("/{document_id}/search", response_model=SearchResponse)
async def search_document(
        document_id: str,
        request: SearchRequest,
        current_user: User = Depends(get_current_user)
):
    """æ–‡æ¡£å¬å›æµ‹è¯• - ä½¿ç”¨ç»Ÿä¸€çš„å¬å›æ–¹æ¡ˆ"""
    start_time = time.time()

    doc = verify_document_ownership(document_id, current_user.username)

    if doc.status != "vectorized":
        raise HTTPException(status_code=400, detail="å¿…é¡»å…ˆå®Œæˆå‘é‡åŒ–")

    try:
        # åŠ è½½ä¸´æ—¶åº“
        vectorstore = Chroma(
            collection_name=doc.chroma_collection_name,
            embedding_function=embedding_model,
            persist_directory=str(CHROMA_TEMP_DIR)
        )

        # â­ï¸ ä½¿ç”¨ç»Ÿä¸€çš„å¬å›å‡½æ•°
        results = retrieve_with_rerank(
            vectorstore=vectorstore,
            query=request.query,
            top_k=request.top_k,
            use_rerank=request.use_rerank,
            threshold=request.threshold
        )

        # è½¬æ¢ä¸ºå“åº”æ ¼å¼
        search_results = []
        for doc_result, similarity in results:
            metadata = doc_result.metadata
            search_results.append(SearchResultItem(
                chunk_id=f"chunk_{metadata['chunk_index']}",
                chunk_index=metadata["chunk_index"],
                content=doc_result.page_content,
                similarity_score=round(similarity, 4),
                char_count=metadata["char_count"],
                start_pos=metadata["start_pos"],
                end_pos=metadata["end_pos"]
            ))

        search_time = (time.time() - start_time) * 1000

        return SearchResponse(
            document_id=document_id,
            query=request.query,
            results=search_results,
            total_results=len(search_results),
            search_time_ms=round(search_time, 2)
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æœç´¢å¤±è´¥: {str(e)}")


@router.post("/{document_id}/confirm", response_model=ConfirmResponse)
async def confirm_document(
        document_id: str,
        current_user: User = Depends(get_current_user)
):
    """
    ç¡®è®¤å…¥åº“ - å°†ä¸´æ—¶æ•°æ®è¿ç§»åˆ°æ­£å¼åº“

    æµç¨‹ï¼š
    1. éªŒè¯æ–‡æ¡£çŠ¶æ€ï¼ˆå¿…é¡»æ˜¯ vectorizedï¼‰
    2. ä»ä¸´æ—¶åº“è¯»å–æ‰€æœ‰æ•°æ®
    3. å†™å…¥æ­£å¼åº“
    4. åˆ é™¤ä¸´æ—¶åº“
    5. æ›´æ–°æ–‡æ¡£çŠ¶æ€ä¸º confirmed
    """
    doc = verify_document_ownership(document_id, current_user.username)

    # 1. æ£€æŸ¥æ–‡æ¡£çŠ¶æ€
    if doc.status != "vectorized":
        raise HTTPException(
            status_code=400,
            detail=f"æ–‡æ¡£çŠ¶æ€é”™è¯¯ï¼Œå½“å‰çŠ¶æ€: {doc.status}ï¼Œå¿…é¡»å…ˆå®Œæˆå‘é‡åŒ–"
        )

    if not doc.chroma_collection_name:
        raise HTTPException(status_code=400, detail="æ–‡æ¡£æœªåˆ›å»ºå‘é‡åº“")

    try:
        # 2. åŠ è½½ä¸´æ—¶collection
        temp_vectorstore = Chroma(
            collection_name=doc.chroma_collection_name,
            embedding_function=embedding_model,
            persist_directory=str(CHROMA_TEMP_DIR)
        )

        # 3. è·å–æ‰€æœ‰æ•°æ®
        temp_collection = temp_vectorstore._collection
        all_data = temp_collection.get(include=['documents', 'metadatas', 'embeddings'])

        if not all_data['ids']:
            raise HTTPException(status_code=400, detail="ä¸´æ—¶åº“ä¸­æ²¡æœ‰æ•°æ®")

        # 4. åˆ›å»ºæ­£å¼åº“collection
        permanent_collection_name = f"doc_{document_id.replace('-', '_')}"

        permanent_client = chromadb.PersistentClient(path=str(CHROMA_PERMANENT_DIR))

        # åˆ é™¤å·²å­˜åœ¨çš„åŒåcollectionï¼ˆå¦‚æœæœ‰ï¼‰
        try:
            permanent_client.delete_collection(name=permanent_collection_name)
        except:
            pass

        # åˆ›å»ºæ–°çš„æ­£å¼collection
        permanent_collection = permanent_client.create_collection(
            name=permanent_collection_name,
            metadata={"hnsw:space": "cosine"}
        )

        # 5. æ·»åŠ æ•°æ®åˆ°æ­£å¼åº“
        permanent_collection.add(
            ids=all_data['ids'],
            documents=all_data['documents'],
            metadatas=all_data['metadatas'],
            embeddings=all_data['embeddings']
        )

        print(f"âœ… æ•°æ®å·²è¿ç§»åˆ°æ­£å¼åº“: {permanent_collection_name}")

        # 6. åˆ é™¤ä¸´æ—¶collection
        cleanup_temp_collection(doc.chroma_collection_name)

        # 7. æ›´æ–°æ–‡æ¡£çŠ¶æ€
        doc.status = "confirmed"
        doc.permanent_collection_name = permanent_collection_name
        doc.confirmed_at = datetime.now()

        # 8. æ¸…ç†ä¸´æ—¶æ•°æ®ï¼ˆå¯é€‰ï¼Œä¿ç•™chunksä¾¿äºæŸ¥çœ‹ï¼‰
        doc.chroma_collection_name = None
        vectorstore_cache.clear_client()

        return ConfirmResponse(
            document_id=document_id,
            status="confirmed",
            permanent_collection_name=permanent_collection_name,
            total_chunks=len(all_data['ids']),
            confirmed_at=doc.confirmed_at.isoformat(),
            message=f"æ–‡æ¡£å·²æˆåŠŸå…¥åº“ï¼Œå…± {len(all_data['ids'])} ä¸ªæ–‡æœ¬å—"
        )

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"ç¡®è®¤å…¥åº“å¤±è´¥: {str(e)}"
        )


@router.get("/{document_id}/permanent", response_model=PermanentDocumentResponse)
async def get_permanent_document(
        document_id: str,
        page: int = 1,
        page_size: int = 10,
        current_user: User = Depends(get_current_user)
):
    """
    æŸ¥çœ‹æ­£å¼åº“ä¸­çš„æ–‡æ¡£å†…å®¹
    - æ”¯æŒåˆ†é¡µæŸ¥çœ‹
    - è¿”å›æ–‡æœ¬å†…å®¹å’Œmetadata
    """
    # éªŒè¯æ–‡æ¡£æ‰€æœ‰æƒ
    doc = verify_document_ownership(document_id, current_user.username)

    # æ£€æŸ¥æ–‡æ¡£çŠ¶æ€
    if doc.status != "confirmed":
        raise HTTPException(
            status_code=400,
            detail=f"æ–‡æ¡£çŠ¶æ€é”™è¯¯ï¼Œå½“å‰çŠ¶æ€: {doc.status}ï¼Œå¿…é¡»å…ˆç¡®è®¤å…¥åº“ï¼ˆçŠ¶æ€ä¸ºconfirmedï¼‰"
        )

    if not doc.permanent_collection_name:
        raise HTTPException(status_code=400, detail="æ–‡æ¡£æœªåˆ›å»ºæ­£å¼åº“")

    try:

        # è¿æ¥æ­£å¼åº“
        client = chromadb.PersistentClient(path=str(CHROMA_PERMANENT_DIR))
        collection = client.get_collection(name=doc.permanent_collection_name)

        # è·å–æ‰€æœ‰æ•°æ®
        all_data = collection.get(include=['documents', 'metadatas'])

        total_chunks = len(all_data['ids'])

        # æŒ‰chunk_indexæ’åº
        chunks_with_metadata = []
        for i in range(total_chunks):
            chunks_with_metadata.append({
                'chunk_id': all_data['ids'][i],
                'content': all_data['documents'][i],
                'metadata': all_data['metadatas'][i]
            })

        # æŒ‰chunk_indexæ’åº
        chunks_with_metadata.sort(key=lambda x: x['metadata']['chunk_index'])

        # åˆ†é¡µ
        start_idx = (page - 1) * page_size
        end_idx = start_idx + page_size

        paginated_chunks = chunks_with_metadata[start_idx:end_idx]

        # æ„å»ºå“åº”
        chunk_items = []
        for item in paginated_chunks:
            metadata = item['metadata']
            chunk_items.append(PermanentChunkItem(
                chunk_id=item['chunk_id'],
                chunk_index=metadata['chunk_index'],
                content=item['content'],
                char_count=metadata['char_count'],
                start_pos=metadata['start_pos'],
                end_pos=metadata['end_pos'],
                metadata=metadata
            ))

        return PermanentDocumentResponse(
            document_id=document_id,
            permanent_collection_name=doc.permanent_collection_name,
            total_chunks=total_chunks,
            chunks=chunk_items,
            page=page,
            page_size=page_size,
            has_more=end_idx < total_chunks
        )

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"æŸ¥è¯¢æ­£å¼åº“å¤±è´¥: {str(e)}"
        )