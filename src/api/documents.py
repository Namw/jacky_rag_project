from fastapi import UploadFile, File, HTTPException, APIRouter, Depends
from fastapi.responses import JSONResponse
from langchain_chroma import Chroma
from langchain_core.messages import SystemMessage, HumanMessage
from pydantic import BaseModel, Field
from typing import List, Optional
import re
import uuid
import os
from pathlib import Path
from datetime import datetime
import fitz  # PyMuPDF
import time
import chromadb
from src.api.auth import get_current_user, User
from src.api.chat import get_llm
from src.services.chroma_cleanup import delete_collection_completely
from src.services.retrieval_service import retrieve_with_rerank, embedding_model, CHROMA_PERMANENT_DIR
from src.services.vector_store_cache import vectorstore_cache

# ä¿®æ”¹routerçš„prefixå’Œtags
router = APIRouter(
    prefix="/api/documents",
    tags=["Documents"],
)

# é…ç½®
UPLOAD_DIR = Path("data/uploads")
UPLOAD_DIR.mkdir(exist_ok=True)
MAX_FILE_SIZE = 5 * 1024 * 1024  # 5MB

# Chromaå­˜å‚¨è·¯å¾„
CHROMA_TEMP_DIR = Path("data/vectorstore/temp")  # ä¸´æ—¶åº“
CHROMA_TEMP_DIR.mkdir(exist_ok=True)
CHROMA_PERMANENT_DIR.mkdir(exist_ok=True)

# ç®€å•çš„å†…å­˜å­˜å‚¨ï¼ˆç”Ÿäº§ç¯å¢ƒåº”è¯¥ç”¨æ•°æ®åº“ï¼‰
documents_db = {}


class Document:
    def __init__(self, document_id: str, filename: str, filepath: str,
                 page_count: int, file_size: int, user_id: str):
        self.document_id = document_id
        self.filename = filename
        self.filepath = filepath
        self.page_count = page_count
        self.file_size = file_size
        self.user_id = user_id
        self.status = "uploaded"
        self.created_at = datetime.now()
        self.text_content = None
        self.chunks = None
        self.chroma_collection_name = None
        self.permanent_collection_name = None  # æ­£å¼åº“collectionåç§°
        self.confirmed_at = None  # ç¡®è®¤æ—¶é—´
        self.category = None
        self.page_char_ranges = None



def cleanup_temp_collection(collection_name: str):
    """åˆ é™¤Chromaä¸´æ—¶collectionï¼ˆåŒ…æ‹¬ç‰©ç†æ–‡ä»¶ï¼‰"""
    delete_result = delete_collection_completely(
        collection_name=collection_name,
        persist_dir=str(CHROMA_TEMP_DIR),
        verbose=True
    )

    if not delete_result["collection_deleted"]:
        print(f"âš ï¸ åˆ é™¤ä¸´æ—¶collectionå¤±è´¥: {collection_name}")


def verify_document_ownership(document_id: str, user_id: str):
    """éªŒè¯æ–‡æ¡£æ‰€æœ‰æƒ"""
    if document_id not in documents_db:
        raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")

    doc = documents_db[document_id]

    if doc.user_id != user_id:
        raise HTTPException(status_code=403, detail="æ— æƒè®¿é—®æ­¤æ–‡æ¡£")

    return doc


# ==================== Pydantic Models ====================

class ChunkRequest(BaseModel):
    """åˆ†å—è¯·æ±‚å‚æ•°"""
    chunk_size: int = Field(default=500, ge=100, le=2000, description="åˆ†å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰")
    overlap: int = Field(default=50, ge=0, le=500, description="é‡å å­—ç¬¦æ•°")
    separator: str = Field(default="\n\n", description="åˆ†éš”ç¬¦")


class ChunkItem(BaseModel):
    """å•ä¸ªåˆ†å—"""
    chunk_id: str
    content: str
    start_pos: int
    end_pos: int
    char_count: int
    index: int


class ChunkResponse(BaseModel):
    """åˆ†å—å“åº”"""
    document_id: str
    chunks: List[ChunkItem]
    total_chunks: int
    total_chars: int
    chunk_size: int
    overlap: int
    category: str


class VectorizeResponse(BaseModel):
    """å‘é‡åŒ–å“åº”"""
    document_id: str
    status: str
    total_chunks: int
    embedding_dim: int
    message: str


class SearchRequest(BaseModel):
    """æœç´¢è¯·æ±‚å‚æ•°"""
    query: str = Field(..., min_length=1, description="æœç´¢é—®é¢˜")
    top_k: int = Field(default=5, ge=1, le=20, description="è¿”å›top-kä¸ªæœ€ç›¸å…³çš„chunks")
    threshold: Optional[float] = Field(default=None, ge=0.0, le=1.0, description="ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆå¯é€‰ï¼‰")
    use_rerank: bool = Field(default=False, description="æ˜¯å¦å¯ç”¨rerankäºŒæ¬¡ç²¾æ’")  # æ–°å¢ â­ï¸
    filter_category: Optional[str] = Field(default=None, description="æŒ‰åˆ†ç±»è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰")  # ğŸ‘ˆ æ–°å¢

class SearchResultItem(BaseModel):
    """å•ä¸ªæœç´¢ç»“æœ"""
    chunk_id: str
    chunk_index: int
    content: str
    similarity_score: float
    char_count: int
    start_pos: int
    end_pos: int


class SearchResponse(BaseModel):
    """æœç´¢å“åº”"""
    document_id: str
    query: str
    results: List[SearchResultItem]
    total_results: int
    search_time_ms: float


class ConfirmResponse(BaseModel):
    """ç¡®è®¤å…¥åº“å“åº”"""
    document_id: str
    status: str
    permanent_collection_name: str
    total_chunks: int
    confirmed_at: str
    message: str

class PermanentChunkItem(BaseModel):
    """æ­£å¼åº“ä¸­çš„å•ä¸ªåˆ†å—"""
    chunk_id: str
    chunk_index: int
    content: str
    char_count: int
    start_pos: int
    end_pos: int
    metadata: dict

class PermanentDocumentResponse(BaseModel):
    """æ­£å¼åº“æ–‡æ¡£æŸ¥çœ‹å“åº”"""
    document_id: str
    permanent_collection_name: str
    total_chunks: int
    chunks: List[PermanentChunkItem]
    page: int
    page_size: int
    has_more: bool

# ==================== å·¥å…·å‡½æ•° ====================

def split_text_with_overlap(text: str, chunk_size: int, overlap: int, separator: str) -> List[dict]:
    """æ–‡æœ¬åˆ†å—å‡½æ•°"""
    chunks = []
    text_length = len(text)
    start = 0
    index = 0

    while start < text_length:
        end = start + chunk_size

        if end < text_length:
            search_start = max(start, end - 100)
            search_end = min(text_length, end + 100)
            search_text = text[search_start:search_end]

            sep_pos = search_text.rfind(separator)
            if sep_pos != -1:
                end = search_start + sep_pos + len(separator)
        else:
            end = text_length

        chunk_content = text[start:end].strip()

        if chunk_content:
            chunk_id = f"chunk_{index}"
            chunks.append({
                "chunk_id": chunk_id,
                "content": chunk_content,
                "start_pos": start,
                "end_pos": end,
                "char_count": len(chunk_content),
                "index": index
            })
            index += 1

        start = end - overlap

        if start >= text_length or (end >= text_length and start == end - overlap):
            break

    return chunks

def extract_category_from_chunks(
        chunks: List[dict],
        max_chunks: int = 3
) -> str:  # ğŸ‘ˆ æ³¨æ„ï¼šè¿”å›å€¼æ”¹ä¸º strï¼Œè€Œä¸æ˜¯ dict
    """
    ä½¿ç”¨ LLM è¿›è¡Œæ–‡æ¡£åˆ†ç±»ï¼ˆè¿”å›åˆ†ç±»åç§°å­—ç¬¦ä¸²ï¼‰

    :param chunks: æ–‡æ¡£å—åˆ—è¡¨
    :param max_chunks: ä½¿ç”¨å‰å‡ ä¸ªchunkè¿›è¡Œåˆ†ç±»
    :return: åˆ†ç±»åç§°å­—ç¬¦ä¸²
    """
    llm = get_llm()

    # æå–æ ·æœ¬æ–‡æœ¬ï¼ˆé™åˆ¶é•¿åº¦é¿å…tokenè¿‡å¤šï¼‰
    sample_text = "\n".join([
        f"ç‰‡æ®µ{i + 1}: {chunk['content'][:300]}"  # æ¯ä¸ªchunkåªå–300å­—
        for i, chunk in enumerate(chunks[:max_chunks])
    ])

    # System Prompt - è‡ªç”±åˆ†ç±»æ¨¡å¼
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†ç±»åŠ©æ‰‹ã€‚
ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®æ–‡æ¡£å†…å®¹ï¼Œä¸ºå…¶ç¡®å®šä¸€ä¸ªæ°å½“çš„åˆ†ç±»æ ‡ç­¾ã€‚

åˆ†ç±»è¦æ±‚ï¼š
1. ä»”ç»†é˜…è¯»æ–‡æ¡£ç‰‡æ®µï¼Œç†è§£å…¶æ ¸å¿ƒå†…å®¹ã€ç”¨é€”å’Œæ€§è´¨
2. ç»™å‡ºä¸€ä¸ªç®€æ´ã€å‡†ç¡®çš„åˆ†ç±»åç§°ï¼ˆ2-6ä¸ªå­—ï¼‰
3. åˆ†ç±»åº”è¯¥æ˜¯é€šç”¨çš„ã€æ ‡å‡†çš„ç±»å‹

å¸¸è§æ–‡æ¡£ç±»å‹å‚è€ƒï¼ˆä½†ä¸é™äºï¼‰ï¼š
- äººäº‹ç±»ï¼šç®€å†ã€å…¥èŒç™»è®°è¡¨ã€ç¦»èŒç”³è¯·ã€å‘˜å·¥èŠ±åå†Œ
- æ³•åŠ¡ç±»ï¼šåŠ³åŠ¨åˆåŒã€ä¿å¯†åè®®ã€æ‰¿è¯ºä¹¦ã€æˆæƒå§”æ‰˜ä¹¦
- ç®¡ç†ç±»ï¼šç®¡ç†åˆ¶åº¦ã€æ“ä½œè§„ç¨‹ã€å·¥ä½œæµç¨‹ã€é€šçŸ¥å…¬å‘Š
- è´¢åŠ¡ç±»ï¼šè´¢åŠ¡æŠ¥è¡¨ã€å‘ç¥¨ã€æ”¶æ®ã€æŠ¥é”€å•
- é¡¹ç›®ç±»ï¼šé¡¹ç›®æ–¹æ¡ˆã€æŠ€æœ¯æ–‡æ¡£ã€éœ€æ±‚æ–‡æ¡£ã€æµ‹è¯•æŠ¥å‘Š
- ä¼šè®®ç±»ï¼šä¼šè®®çºªè¦ã€ä¼šè®®é€šçŸ¥ã€ä¼šè®®è®®ç¨‹

è¯·ä»”ç»†åˆ†ææ–‡æ¡£å†…å®¹åï¼Œåªè¿”å›ä¸€ä¸ªæœ€åˆé€‚çš„åˆ†ç±»åç§°ï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹ã€‚
ä¾‹å¦‚ï¼šç®€å†ã€åŠ³åŠ¨åˆåŒã€ç®¡ç†åˆ¶åº¦ã€ä¼šè®®çºªè¦ç­‰ã€‚"""

    user_prompt = f"""è¯·å¯¹ä»¥ä¸‹æ–‡æ¡£è¿›è¡Œåˆ†ç±»ï¼Œåªè¿”å›åˆ†ç±»åç§°ï¼ˆ2-6ä¸ªå­—ï¼‰ï¼š

{sample_text}

åˆ†ç±»åç§°ï¼š"""

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt)
    ]

    try:
        response = llm.invoke(messages)
        category = response.content.strip()

        # æ¸…ç†å¯èƒ½çš„å¤šä½™å†…å®¹
        # ç§»é™¤å¯èƒ½çš„æ ‡ç‚¹ç¬¦å·ã€å¼•å·ç­‰
        category = category.replace('"', '').replace("'", "").replace('ã€‚', '').strip()

        # å¦‚æœè¿”å›å†…å®¹è¿‡é•¿ï¼Œå°è¯•æå–ç¬¬ä¸€ä¸ªæœ‰æ•ˆåˆ†ç±»è¯
        if len(category) > 10:
            # å°è¯•åŒ¹é…å¸¸è§æ¨¡å¼
            match = re.search(
                r'(ç®€å†|åˆåŒ|åˆ¶åº¦|æŠ¥è¡¨|çºªè¦|æ–¹æ¡ˆ|æ–‡æ¡£|åè®®|é€šçŸ¥|ç”³è¯·|ç™»è®°|æ‰¿è¯ºä¹¦|å§”æ‰˜ä¹¦|æŠ¥é”€å•|å‘ç¥¨|æ”¶æ®)', category)
            if match:
                category = match.group(1)
            else:
                category = category[:6]  # æˆªå–å‰6ä¸ªå­—

        # å¦‚æœä¸ºç©ºï¼Œè¿”å›é»˜è®¤å€¼
        if not category:
            category = "å…¶ä»–æ–‡æ¡£"

        return category

    except Exception as e:
        print(f"LLMåˆ†ç±»å¤±è´¥: {str(e)}")
        return "æœªåˆ†ç±»"  # ğŸ‘ˆ é”™è¯¯æ—¶è¿”å›é»˜è®¤å­—ç¬¦ä¸²


def get_page_numbers(start_pos: int, end_pos: int, page_char_ranges: List[dict]) -> List[int]:
    """
    æ ¹æ®å­—ç¬¦èŒƒå›´ç¡®å®šæ‰€æœ‰æ¶‰åŠçš„é¡µç 

    è¿”å›: [1, 2] è¡¨ç¤ºåˆ†å—è·¨è¶Šç¬¬1é¡µå’Œç¬¬2é¡µ
    """
    pages = set()

    for page_info in page_char_ranges:
        page_start = page_info["start_char"]
        page_end = page_info["end_char"]

        # åˆ¤æ–­åˆ†å—ä¸é¡µé¢æ˜¯å¦æœ‰é‡å 
        if not (end_pos <= page_start or start_pos >= page_end):
            pages.add(page_info["page_num"])

    return sorted(list(pages))

# ==================== API Endpoints ====================

@router.post("/upload")
async def upload_document(
        file: UploadFile = File(...),
        current_user: User = Depends(get_current_user)
):
    """ä¸Šä¼ PDFæ–‡ä»¶"""

    if not file.filename.endswith('.pdf'):
        raise HTTPException(status_code=400, detail="åªæ”¯æŒPDFæ–‡ä»¶")

    content = await file.read()
    file_size = len(content)

    if file_size > MAX_FILE_SIZE:
        raise HTTPException(
            status_code=400,
            detail=f"æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶ï¼ˆæœ€å¤§5MBï¼‰ï¼Œå½“å‰æ–‡ä»¶: {file_size / 1024 / 1024:.2f}MB"
        )

    try:
        document_id = str(uuid.uuid4())
        filepath = UPLOAD_DIR / f"{document_id}.pdf"

        with open(filepath, "wb") as f:
            f.write(content)

        doc_pdf = fitz.open(filepath)
        page_count = doc_pdf.page_count

        text_content = ""
        page_char_ranges = []

        for page_num in range(page_count):
            page = doc_pdf[page_num]
            page_text = page.get_text()

            start_char = len(text_content)
            text_content += page_text + "\n"
            end_char = len(text_content)

            page_char_ranges.append({
                "page_num": page_num + 1,  # é¡µç ä»1å¼€å§‹
                "start_char": start_char,
                "end_char": end_char
            })

        doc_pdf.close()

        if not text_content.strip():
            os.remove(filepath)
            raise HTTPException(status_code=400, detail="PDFæ–‡ä»¶æ— æ³•æå–æ–‡æœ¬å†…å®¹")

    except Exception as e:
        if filepath.exists():
            os.remove(filepath)
        raise HTTPException(status_code=400, detail=f"PDFæ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}")

    doc = Document(
        document_id=document_id,
        filename=file.filename,
        filepath=str(filepath),
        page_count=page_count,
        file_size=file_size,
        user_id=current_user.username  # ä½¿ç”¨ username
    )
    doc.text_content = text_content
    doc.page_char_ranges = page_char_ranges  # ğŸ‘ˆ ä¿å­˜é¡µç æ˜ å°„

    documents_db[document_id] = doc

    return JSONResponse(
        status_code=200,
        content={
            "document_id": document_id,
            "filename": file.filename,
            "file_size": file_size,
            "page_count": page_count,
            "status": "uploaded",
            "created_at": doc.created_at.isoformat()
        }
    )

@router.post("/{document_id}/chunk", response_model=ChunkResponse)
async def chunk_document(
        document_id: str,
        request: ChunkRequest,
        current_user: User = Depends(get_current_user)
):
    """å¯¹æ–‡æ¡£è¿›è¡Œåˆ†å—ï¼ˆå…è®¸é‡å¤åˆ†å—ï¼Œè‡ªåŠ¨è¦†ç›–æ—§æ•°æ®ï¼‰"""
    doc = verify_document_ownership(document_id, current_user.username)

    if doc.status not in ["uploaded", "chunked"]:
        raise HTTPException(
            status_code=400,
            detail=f"æ–‡æ¡£çŠ¶æ€é”™è¯¯ï¼Œå½“å‰çŠ¶æ€: {doc.status}ï¼Œåªèƒ½å¯¹ uploaded æˆ– chunked çŠ¶æ€çš„æ–‡æ¡£è¿›è¡Œåˆ†å—"
        )

    if not doc.text_content:
        raise HTTPException(status_code=400, detail="æ–‡æ¡£æ²¡æœ‰æ–‡æœ¬å†…å®¹")

    if request.overlap >= request.chunk_size:
        raise HTTPException(status_code=400, detail="overlapä¸èƒ½å¤§äºæˆ–ç­‰äºchunk_size")

    try:
        chunks = split_text_with_overlap(
            text=doc.text_content,
            chunk_size=request.chunk_size,
            overlap=request.overlap,
            separator=request.separator
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ†å—å¤±è´¥: {str(e)}")

    # ğŸ‘‡ æ–°å¢ï¼šè‡ªåŠ¨æå–åˆ†ç±»
    category = extract_category_from_chunks(chunks, max_chunks=3)

    doc.category = category  # ğŸ‘ˆ ä¿å­˜åˆ†ç±»
    doc.chunks = chunks
    doc.status = "chunked"

    return ChunkResponse(
        document_id=document_id,
        chunks=[ChunkItem(**chunk) for chunk in chunks],
        total_chunks=len(chunks),
        total_chars=len(doc.text_content),
        chunk_size=request.chunk_size,
        overlap=request.overlap,
        category=category  # ğŸ‘ˆ è¿”å›ç»™å‰ç«¯
    )


@router.post("/{document_id}/vectorize", response_model=VectorizeResponse)
async def vectorize_document(
        document_id: str,
        current_user: User = Depends(get_current_user)
):
    """å¯¹æ–‡æ¡£åˆ†å—è¿›è¡Œå‘é‡åŒ–ï¼ˆå…è®¸é‡å¤å‘é‡åŒ–ï¼Œè‡ªåŠ¨è¦†ç›–æ—§æ•°æ®ï¼‰"""
    doc = verify_document_ownership(document_id, current_user.username)

    if doc.status not in ["chunked", "vectorized"]:
        raise HTTPException(
            status_code=400,
            detail=f"æ–‡æ¡£çŠ¶æ€é”™è¯¯ï¼Œå½“å‰çŠ¶æ€: {doc.status}ï¼Œå¿…é¡»å…ˆå®Œæˆåˆ†å—"
        )

    if not doc.chunks or len(doc.chunks) == 0:
        raise HTTPException(status_code=400, detail="æ–‡æ¡£æ²¡æœ‰åˆ†å—æ•°æ®")

    # å¦‚æœå·²ç»å‘é‡åŒ–è¿‡ï¼Œå…ˆæ¸…ç†æ—§çš„Chroma collection
    if doc.status == "vectorized" and doc.chroma_collection_name:
        try:
            cleanup_temp_collection(doc.chroma_collection_name)
            print(f"âœ… å·²æ¸…ç†æ—§çš„å‘é‡æ•°æ®: {doc.chroma_collection_name}")
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†æ—§collectionå¤±è´¥ï¼ˆå¿½ç•¥ï¼‰: {e}")

    chunk_texts = [chunk["content"] for chunk in doc.chunks]
    chunk_ids = [chunk["chunk_id"] for chunk in doc.chunks]

    metadatas = [
        {
            "chunk_index": chunk["index"],
            "char_count": chunk["char_count"],
            "start_pos": chunk["start_pos"],
            "end_pos": chunk["end_pos"],
            "page_numbers": ",".join(map(str, get_page_numbers(
                chunk["start_pos"],
                chunk["end_pos"],
                doc.page_char_ranges
            ))),
            "document_id": document_id,
            "category": doc.category if doc.category else "æœªåˆ†ç±»"
        }
        for chunk in doc.chunks
    ]

    try:
        collection_name = f"temp_{document_id.replace('-', '_')}"

        Chroma.from_texts(
            texts=chunk_texts,
            embedding=embedding_model,
            ids=chunk_ids,
            metadatas=metadatas,
            collection_name=collection_name,
            persist_directory=str(CHROMA_TEMP_DIR),
            collection_metadata={"hnsw:space": "cosine"}
        )

        doc.chroma_collection_name = collection_name

        sample_embedding = embedding_model.embed_query(chunk_texts[0])
        embedding_dim = len(sample_embedding)

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"å‘é‡åŒ–å¤±è´¥: {str(e)}"
        )

    doc.status = "vectorized"

    return VectorizeResponse(
        document_id=document_id,
        status="vectorized",
        total_chunks=len(doc.chunks),
        embedding_dim=embedding_dim,
        message=f"æˆåŠŸå‘é‡åŒ– {len(doc.chunks)} ä¸ªæ–‡æœ¬å—å¹¶å­˜å…¥Chromaä¸´æ—¶åº“"
    )


@router.post("/{document_id}/search", response_model=SearchResponse)
async def search_document(
        document_id: str,
        request: SearchRequest,
        current_user: User = Depends(get_current_user)
):
    """æ–‡æ¡£å¬å›æµ‹è¯• - ä½¿ç”¨ç»Ÿä¸€çš„å¬å›æ–¹æ¡ˆ"""
    start_time = time.time()

    doc = verify_document_ownership(document_id, current_user.username)

    if doc.status != "vectorized":
        raise HTTPException(status_code=400, detail="å¿…é¡»å…ˆå®Œæˆå‘é‡åŒ–")

    try:
        # åŠ è½½ä¸´æ—¶åº“
        vectorstore = Chroma(
            collection_name=doc.chroma_collection_name,
            embedding_function=embedding_model,
            persist_directory=str(CHROMA_TEMP_DIR)
        )

        # â­ï¸ ä½¿ç”¨ç»Ÿä¸€çš„å¬å›å‡½æ•°
        results = retrieve_with_rerank(
            vectorstore=vectorstore,
            query=request.query,
            top_k=request.top_k,
            use_rerank=request.use_rerank,
            threshold=request.threshold
        )

        # è½¬æ¢ä¸ºå“åº”æ ¼å¼
        search_results = []
        for doc_result, similarity in results:
            metadata = doc_result.metadata
            search_results.append(SearchResultItem(
                chunk_id=f"chunk_{metadata['chunk_index']}",
                chunk_index=metadata["chunk_index"],
                content=doc_result.page_content,
                similarity_score=round(similarity, 4),
                char_count=metadata["char_count"],
                start_pos=metadata["start_pos"],
                end_pos=metadata["end_pos"]
            ))

        search_time = (time.time() - start_time) * 1000

        return SearchResponse(
            document_id=document_id,
            query=request.query,
            results=search_results,
            total_results=len(search_results),
            search_time_ms=round(search_time, 2)
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æœç´¢å¤±è´¥: {str(e)}")


@router.post("/{document_id}/confirm", response_model=ConfirmResponse)
async def confirm_document(
        document_id: str,
        current_user: User = Depends(get_current_user)
):
    """
    ç¡®è®¤å…¥åº“ - å°†ä¸´æ—¶æ•°æ®è¿ç§»åˆ°æ­£å¼åº“

    æµç¨‹ï¼š
    1. éªŒè¯æ–‡æ¡£çŠ¶æ€ï¼ˆå¿…é¡»æ˜¯ vectorizedï¼‰
    2. ä»ä¸´æ—¶åº“è¯»å–æ‰€æœ‰æ•°æ®
    3. å†™å…¥æ­£å¼åº“
    4. åˆ é™¤ä¸´æ—¶åº“
    5. æ›´æ–°æ–‡æ¡£çŠ¶æ€ä¸º confirmed
    """
    doc = verify_document_ownership(document_id, current_user.username)

    # 1. æ£€æŸ¥æ–‡æ¡£çŠ¶æ€
    if doc.status != "vectorized":
        raise HTTPException(
            status_code=400,
            detail=f"æ–‡æ¡£çŠ¶æ€é”™è¯¯ï¼Œå½“å‰çŠ¶æ€: {doc.status}ï¼Œå¿…é¡»å…ˆå®Œæˆå‘é‡åŒ–"
        )

    if not doc.chroma_collection_name:
        raise HTTPException(status_code=400, detail="æ–‡æ¡£æœªåˆ›å»ºå‘é‡åº“")

    try:
        # 2. åŠ è½½ä¸´æ—¶collection
        temp_vectorstore = Chroma(
            collection_name=doc.chroma_collection_name,
            embedding_function=embedding_model,
            persist_directory=str(CHROMA_TEMP_DIR)
        )

        # 3. è·å–æ‰€æœ‰æ•°æ®
        temp_collection = temp_vectorstore._collection
        all_data = temp_collection.get(include=['documents', 'metadatas', 'embeddings'])

        if not all_data['ids']:
            raise HTTPException(status_code=400, detail="ä¸´æ—¶åº“ä¸­æ²¡æœ‰æ•°æ®")

        # 4. åˆ›å»ºæ­£å¼åº“collection
        permanent_collection_name = f"doc_{document_id.replace('-', '_')}"

        permanent_client = chromadb.PersistentClient(path=str(CHROMA_PERMANENT_DIR))

        try:
            # åˆ é™¤å·²å­˜åœ¨çš„åŒåcollectionï¼ˆå¦‚æœæœ‰ï¼‰
            permanent_client.delete_collection(name=permanent_collection_name)
        except:
            pass

        # åˆ›å»ºæ–°çš„æ­£å¼collection
        permanent_collection = permanent_client.create_collection(
            name=permanent_collection_name,
            metadata={"hnsw:space": "cosine"}
        )

        # 5. æ·»åŠ æ•°æ®åˆ°æ­£å¼åº“
        permanent_collection.add(
            ids=all_data['ids'],
            documents=all_data['documents'],
            metadatas=all_data['metadatas'],
            embeddings=all_data['embeddings']
        )

        print(f"âœ… æ•°æ®å·²è¿ç§»åˆ°æ­£å¼åº“: {permanent_collection_name}")

        # 6. åˆ é™¤ä¸´æ—¶collection
        cleanup_temp_collection(doc.chroma_collection_name)

        # 7. æ›´æ–°æ–‡æ¡£çŠ¶æ€
        doc.status = "confirmed"
        doc.permanent_collection_name = permanent_collection_name
        doc.confirmed_at = datetime.now()

        # 8. æ¸…ç†ä¸´æ—¶æ•°æ®ï¼ˆå¯é€‰ï¼Œä¿ç•™chunksä¾¿äºæŸ¥çœ‹ï¼‰
        doc.chroma_collection_name = None
        vectorstore_cache.clear_client()

        return ConfirmResponse(
            document_id=document_id,
            status="confirmed",
            permanent_collection_name=permanent_collection_name,
            total_chunks=len(all_data['ids']),
            confirmed_at=doc.confirmed_at.isoformat(),
            message=f"æ–‡æ¡£å·²æˆåŠŸå…¥åº“ï¼Œå…± {len(all_data['ids'])} ä¸ªæ–‡æœ¬å—"
        )

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"ç¡®è®¤å…¥åº“å¤±è´¥: {str(e)}"
        )


@router.get("/{document_id}/permanent", response_model=PermanentDocumentResponse)
async def get_permanent_document(
        document_id: str,
        page: int = 1,
        page_size: int = 10,
        current_user: User = Depends(get_current_user)
):
    """
    æŸ¥çœ‹æ­£å¼åº“ä¸­çš„æ–‡æ¡£å†…å®¹
    - æ”¯æŒåˆ†é¡µæŸ¥çœ‹
    - è¿”å›æ–‡æœ¬å†…å®¹å’Œmetadata
    """
    # éªŒè¯æ–‡æ¡£æ‰€æœ‰æƒ
    doc = verify_document_ownership(document_id, current_user.username)

    # æ£€æŸ¥æ–‡æ¡£çŠ¶æ€
    if doc.status != "confirmed":
        raise HTTPException(
            status_code=400,
            detail=f"æ–‡æ¡£çŠ¶æ€é”™è¯¯ï¼Œå½“å‰çŠ¶æ€: {doc.status}ï¼Œå¿…é¡»å…ˆç¡®è®¤å…¥åº“ï¼ˆçŠ¶æ€ä¸ºconfirmedï¼‰"
        )

    if not doc.permanent_collection_name:
        raise HTTPException(status_code=400, detail="æ–‡æ¡£æœªåˆ›å»ºæ­£å¼åº“")

    try:

        # è¿æ¥æ­£å¼åº“
        client = chromadb.PersistentClient(path=str(CHROMA_PERMANENT_DIR))
        collection = client.get_collection(name=doc.permanent_collection_name)

        # è·å–æ‰€æœ‰æ•°æ®
        all_data = collection.get(include=['documents', 'metadatas'])

        total_chunks = len(all_data['ids'])

        # æŒ‰chunk_indexæ’åº
        chunks_with_metadata = []
        for i in range(total_chunks):
            chunks_with_metadata.append({
                'chunk_id': all_data['ids'][i],
                'content': all_data['documents'][i],
                'metadata': all_data['metadatas'][i]
            })

        # æŒ‰chunk_indexæ’åº
        chunks_with_metadata.sort(key=lambda x: x['metadata']['chunk_index'])

        # åˆ†é¡µ
        start_idx = (page - 1) * page_size
        end_idx = start_idx + page_size

        paginated_chunks = chunks_with_metadata[start_idx:end_idx]

        # æ„å»ºå“åº”
        chunk_items = []
        for item in paginated_chunks:
            metadata = item['metadata']
            chunk_items.append(PermanentChunkItem(
                chunk_id=item['chunk_id'],
                chunk_index=metadata['chunk_index'],
                content=item['content'],
                char_count=metadata['char_count'],
                start_pos=metadata['start_pos'],
                end_pos=metadata['end_pos'],
                metadata=metadata
            ))

        return PermanentDocumentResponse(
            document_id=document_id,
            permanent_collection_name=doc.permanent_collection_name,
            total_chunks=total_chunks,
            chunks=chunk_items,
            page=page,
            page_size=page_size,
            has_more=end_idx < total_chunks
        )

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"æŸ¥è¯¢æ­£å¼åº“å¤±è´¥: {str(e)}"
        )