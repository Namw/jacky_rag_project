from fastapi import APIRouter, Depends, HTTPException, status
from pydantic import BaseModel, Field
from typing import Optional, List, Tuple
from datetime import datetime
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
from src.services.retrieval_service import get_semantic_cache
from src.services.vector_store_cache import vectorstore_cache
from src.vectorstore.chroma_store import VectorStoreManager
from models.model_factory import ModelFactory
from config.model_config import ModelProvider
from src.api.auth import get_current_user, User
from src.services.retrieval_service import retrieve_with_cache
from src.services.usage_limiter import get_usage_limiter
from langchain_core.documents import Document

# --- 1. å…¨å±€å˜é‡å’Œé…ç½® ---

# å‘é‡æ•°æ®åº“å’Œ LLM å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
_vectorstore: Optional[VectorStoreManager] = None
_llm: Optional[ChatOpenAI] = None
_current_provider: ModelProvider = ModelProvider.DEEPSEEK
_default_top_k: int = 5
_default_temperature: float = 0.7
# --- å…¨å±€ç¼“å­˜å®ä¾‹ ---
_vectorstore_cache = None
# é»˜è®¤ç³»ç»Ÿæç¤ºè¯
DEFAULT_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åŠ©æ‰‹ã€‚

ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®æä¾›çš„å‚è€ƒèµ„æ–™å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

è¦æ±‚ï¼š
1. ä»”ç»†é˜…è¯»å‚è€ƒèµ„æ–™ï¼ŒåŸºäºèµ„æ–™å†…å®¹å›ç­”
2. å¦‚æœèµ„æ–™ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯šå®åœ°å‘Šè¯‰ç”¨æˆ·"æ ¹æ®æä¾›çš„èµ„æ–™ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜"
3. å›ç­”è¦å‡†ç¡®ã€ç®€æ´ã€æœ‰æ¡ç†
4. å¯ä»¥é€‚å½“å¼•ç”¨èµ„æ–™ä¸­çš„å…³é”®ä¿¡æ¯
5. ä¸è¦ç¼–é€ èµ„æ–™ä¸­æ²¡æœ‰çš„å†…å®¹"""

def get_llm() -> ChatOpenAI:
    """è·å– LLM å®ä¾‹"""
    global _llm
    if _llm is None:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="LLM not initialized. Please initialize the service first."
        )
    return _llm


def init_chat_service(
    model_provider: ModelProvider = ModelProvider.DEEPSEEK,
    top_k: int = 5,
    temperature: float = 0.7
):
    """
    åˆå§‹åŒ–èŠå¤©æœåŠ¡

    :param vectorstore_manager: å‘é‡æ•°æ®åº“ç®¡ç†å™¨
    :param model_provider: LLM æä¾›å•†
    :param top_k: é»˜è®¤æ£€ç´¢æ–‡æ¡£æ•°é‡
    :param temperature: LLM æ¸©åº¦
    """
    global _llm, _vectorstore_cache, _current_provider, _default_top_k, _default_temperature
    _vectorstore_cache = vectorstore_cache
    _current_provider = model_provider
    _default_top_k = top_k
    _default_temperature = temperature

    # åˆå§‹åŒ– LLM
    _llm = ModelFactory.create_model(
        provider=model_provider,
        temperature=temperature
    )


# --- 2. Pydantic æ¨¡å‹ ---

class ChatRequest(BaseModel):
    """èŠå¤©è¯·æ±‚æ¨¡å‹"""
    question: str = Field(..., min_length=1, max_length=1000)
    document_id: Optional[str] = Field(None, description="æ–‡æ¡£IDï¼Œä¸ºç©ºåˆ™æ£€ç´¢æ‰€æœ‰æ–‡æ¡£")
    top_k: Optional[int] = Field(None, ge=1, le=20)
    use_rerank: bool = Field(False, description="æ˜¯å¦å¯ç”¨ rerank")  # â­ï¸ æ–°å¢
    threshold: Optional[float] = Field(None, ge=0.0, le=1.0)  # â­ï¸ æ–°å¢
    return_sources: bool = Field(True)
    system_prompt: Optional[str] = Field(None)

class SourceInfo(BaseModel):
    """æ¥æºä¿¡æ¯æ¨¡å‹"""
    source: str = Field(..., description="æ–‡æ¡£æ¥æº")
    page: str = Field(..., description="é¡µç ")
    content: str = Field(..., description="ç›¸å…³å†…å®¹æ‘˜è¦")
    score: float = Field(..., description="ç›¸ä¼¼åº¦åˆ†æ•°")

class ChatResponse(BaseModel):
    """èŠå¤©å“åº”æ¨¡å‹"""
    answer: str = Field(..., description="å›ç­”å†…å®¹")
    question: str = Field(..., description="åŸå§‹é—®é¢˜")
    sources: Optional[List[SourceInfo]] = Field(None, description="å‚è€ƒæ¥æºåˆ—è¡¨")
    timestamp: str = Field(..., description="å“åº”æ—¶é—´æˆ³")
    model_provider: str = Field(..., description="ä½¿ç”¨çš„æ¨¡å‹æä¾›å•†")

    class Config:
        json_schema_extra = {
            "example": {
                "answer": "æ ¹æ®åŠ³åŠ¨åˆåŒï¼ŒæœŸé™ä¸ºä¸‰å¹´...",
                "question": "åŠ³åŠ¨åˆåŒçš„æœŸé™æ˜¯å¤šä¹…ï¼Ÿ",
                "sources": [
                    {
                        "source": "labor_contract.pdf",
                        "page": "1",
                        "content": "åˆåŒæœŸé™ä¸ºä¸‰å¹´...",
                        "score": 0.85
                    }
                ],
                "timestamp": "2025-11-05T10:30:00",
                "model_provider": "deepseek"
            }
        }


class ModelSwitchRequest(BaseModel):
    """æ¨¡å‹åˆ‡æ¢è¯·æ±‚"""
    provider: str = Field(..., description="æ¨¡å‹æä¾›å•†åç§°")
    temperature: Optional[float] = Field(None, description="LLM æ¸©åº¦", ge=0.0, le=2.0)

    class Config:
        json_schema_extra = {
            "example": {
                "provider": "deepseek",
                "temperature": 0.7
            }
        }


class ModelInfo(BaseModel):
    """æ¨¡å‹ä¿¡æ¯"""
    current_provider: str = Field(..., description="å½“å‰ä½¿ç”¨çš„æ¨¡å‹æä¾›å•†")
    available_providers: List[str] = Field(..., description="å¯ç”¨çš„æ¨¡å‹æä¾›å•†åˆ—è¡¨")
    temperature: float = Field(..., description="å½“å‰æ¸©åº¦è®¾ç½®")



# --- 3. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ---
def retrieve_documents(
        document_id: Optional[str],
        query: str,
        k: int,
        use_rerank: bool = False,
        threshold: Optional[float] = None
) -> List[Tuple[Document, float]]:
    """æ£€ç´¢æ–‡æ¡£ï¼ˆä½¿ç”¨å¸¦ TTL çš„ç¼“å­˜ï¼‰"""

    # 1. è·å–å®¢æˆ·ç«¯
    client = _vectorstore_cache.get_client()

    # 2. è·å– collection åˆ—è¡¨
    if document_id:
        collection_names = [f"doc_{document_id.replace('-', '_')}"]
    else:
        all_collections = client.list_collections()
        collection_names = [col.name for col in all_collections]

    # 3. â­ï¸ ä»ç¼“å­˜ä¸­è·å– vectorstore
    all_results = []
    for collection_name in collection_names:
        try:
            vectorstore = _vectorstore_cache.get(collection_name)

            results = retrieve_with_cache(
                vectorstore=vectorstore,
                query=query,
                collection_name=collection_name,
                top_k=k,
                use_rerank=use_rerank,
                threshold=threshold
            )

            all_results.extend(results)

        except Exception as e:
            print(f"Warning: Failed to retrieve from {collection_name}: {str(e)}")
            continue

    all_results.sort(key=lambda x: x[1], reverse=True)
    return all_results[:k] if document_id else all_results[:k * 2]


def build_prompt(
        query: str,
        documents: List[tuple],
        system_prompt: Optional[str] = None
) -> tuple[str, str]:
    """æ„å»º Prompt"""
    sys_prompt = system_prompt or DEFAULT_SYSTEM_PROMPT

    context_parts = []
    for i, (doc, score) in enumerate(documents, 1):
        source = doc.metadata.get('category', 'Unknown')  # ğŸ‘ˆ æ”¹ç”¨ category

        # ğŸ‘‡ å¤„ç†é¡µç åˆ—è¡¨
        page_numbers = doc.metadata.get('page_numbers', [])
        if isinstance(page_numbers, list) and page_numbers:
            if len(page_numbers) == 1:
                page_str = f"ç¬¬{format_page_numbers(doc.metadata.get('page_numbers'))}é¡µ"
            else:
                page_str = f"ç¬¬{page_numbers[0]}-{page_numbers[-1]}é¡µ"
        else:
            page_str = "é¡µç æœªçŸ¥"

        content = doc.page_content

        context_parts.append(
            f"ã€å‚è€ƒèµ„æ–™ {i}ã€‘\n"
            f"æ¥æº: {source} ({page_str})\n"  # ğŸ‘ˆ æ ¼å¼åŒ–åçš„é¡µç 
            f"å†…å®¹: {content}\n"
        )

    context = "\n".join(context_parts)

    user_message = f"""å‚è€ƒèµ„æ–™ï¼š
{context}

---

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·åŸºäºä¸Šè¿°å‚è€ƒèµ„æ–™å›ç­”é—®é¢˜ã€‚"""

    return sys_prompt, user_message

def generate_answer(
    query: str,
    documents: List[tuple],
    system_prompt: Optional[str] = None
) -> str:
    """
    ç”Ÿæˆç­”æ¡ˆ

    :param query: ç”¨æˆ·é—®é¢˜
    :param documents: æ£€ç´¢åˆ°çš„æ–‡æ¡£
    :param system_prompt: è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯
    :return: LLM ç”Ÿæˆçš„ç­”æ¡ˆ
    """
    llm = get_llm()

    # æ„å»º Prompt
    sys_msg, user_msg = build_prompt(query, documents, system_prompt)

    # è°ƒç”¨ LLM
    messages = [
        SystemMessage(content=sys_msg),
        HumanMessage(content=user_msg)
    ]

    try:
        response = llm.invoke(messages)
        answer = response.content
        return answer
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"LLM generation failed: {str(e)}"
        )


def format_page_numbers(page_numbers) -> str:
    """
    æ ¼å¼åŒ–é¡µç æ˜¾ç¤º

    :param page_numbers: é¡µç åˆ—è¡¨æˆ–å…¶ä»–æ ¼å¼
    :return: æ ¼å¼åŒ–çš„é¡µç å­—ç¬¦ä¸²

    ç¤ºä¾‹:
        [5] -> "5"
        [3, 4] -> "3-4"
        [1, 2, 3] -> "1-3"
        [] -> "N/A"
    """
    if not page_numbers:
        return "N/A"

    if isinstance(page_numbers, list):
        if len(page_numbers) == 1:
            return str(page_numbers[0])
        else:
            return f"{page_numbers[0]}-{page_numbers[-1]}"

    # å…¼å®¹æ—§æ•°æ®ï¼ˆå¦‚æœæ˜¯å•ä¸ªæ•°å­—ï¼‰
    return str(page_numbers)

# --- 4. è·¯ç”± ---

router = APIRouter(
    prefix="/api/chat",
    tags=["Chat"],
)


@router.post("/query", response_model=ChatResponse)
async def chat_query(
        request: ChatRequest,
        current_user: User = Depends(get_current_user)
):
    """RAG é—®ç­”æ¥å£ - ä½¿ç”¨ç»Ÿä¸€çš„å¬å›æ–¹æ¡ˆ"""
    # æ£€æŸ¥é—®ç­”é™é¢ â­ï¸ æ–°å¢
    limiter = get_usage_limiter()
    can_query, error_msg = limiter.check_can_query(current_user.id)
    if not can_query:
        raise HTTPException(
            status_code=429,
            detail=error_msg
        )

    try:
        k = request.top_k or _default_top_k

        # 1. æ£€ç´¢æ–‡æ¡£ï¼ˆä½¿ç”¨ç»Ÿä¸€çš„å¬å›é€»è¾‘ï¼‰
        documents = retrieve_documents(
            document_id=request.document_id,  # â­ï¸
            query=request.question,
            k=k,
            use_rerank=request.use_rerank,  # â­ï¸
            threshold=request.threshold  # â­ï¸
        )

        if not documents:
            return ChatResponse(
                answer="æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„å‚è€ƒèµ„æ–™ã€‚",
                question=request.question,
                sources=None,
                timestamp=datetime.now().isoformat(),
                model_provider=_current_provider.value
            )

        # 2. ç”Ÿæˆç­”æ¡ˆï¼ˆé€»è¾‘ä¸å˜ï¼‰
        answer = generate_answer(
            query=request.question,
            documents=documents,
            system_prompt=request.system_prompt
        )

        # 3. æ„å»ºå“åº”
        sources = None
        if request.return_sources:
            sources = []
            for doc, score in documents:
                # ğŸ‘‡ æ ¼å¼åŒ–é¡µç 
                sources.append(SourceInfo(
                    source=doc.metadata.get('category', 'Unknown'),
                    page=format_page_numbers(doc.metadata.get('page_numbers')),
                    content=doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                    score=float(score)
                ))

        # å¢åŠ é—®ç­”è®¡æ•° â­ï¸ æ–°å¢
        limiter.increment_query(current_user.id)

        return ChatResponse(
            answer=answer,
            question=request.question,
            sources=sources,
            timestamp=datetime.now().isoformat(),
            model_provider=_current_provider.value
        )

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Query failed: {str(e)}"
        )

@router.post("/switch-model")
async def switch_model(
    request: ModelSwitchRequest,
    current_user: User = Depends(get_current_user)
):
    """
    åˆ‡æ¢ LLM æ¨¡å‹æä¾›å•†

    éœ€è¦æœ‰æ•ˆçš„ Token æ‰èƒ½è®¿é—®
    ä»…ç®¡ç†å‘˜å¯ä»¥åˆ‡æ¢æ¨¡å‹
    """
    # æ£€æŸ¥æƒé™
    if current_user.role != "admin":
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Only admin can switch models"
        )

    try:
        global _llm, _current_provider, _default_temperature

        # éªŒè¯æä¾›å•†
        try:
            provider = ModelProvider(request.provider)
        except ValueError:
            available = [p.value for p in ModelProvider]
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Invalid provider. Available providers: {available}"
            )

        # ä½¿ç”¨è¯·æ±‚ä¸­çš„æ¸©åº¦æˆ–å½“å‰æ¸©åº¦
        temperature = request.temperature if request.temperature is not None else _default_temperature

        # åˆ›å»ºæ–°çš„ LLM å®ä¾‹
        _llm = ModelFactory.create_model(
            provider=provider,
            temperature=temperature
        )
        _current_provider = provider
        _default_temperature = temperature

        return {
            "message": f"Successfully switched to {provider.value}",
            "current_provider": provider.value,
            "temperature": temperature
        }

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to switch model: {str(e)}"
        )

@router.get("/model-info", response_model=ModelInfo)
async def get_model_info(
    current_user: User = Depends(get_current_user)
):
    """
    è·å–å½“å‰æ¨¡å‹ä¿¡æ¯

    éœ€è¦æœ‰æ•ˆçš„ Token æ‰èƒ½è®¿é—®
    """
    return ModelInfo(
        current_provider=_current_provider.value,
        available_providers=[p.value for p in ModelProvider],
        temperature=_default_temperature
    )

@router.get("/cache/stats")
async def get_cache_stats(current_user: User = Depends(get_current_user)):
    """è·å–ç¼“å­˜ç»Ÿè®¡"""
    from src.services.retrieval_service import get_semantic_cache

    cache = get_semantic_cache()
    if not cache:
        return {"error": "Cache not initialized"}

    stats = cache.get_cache_stats()
    return stats


@router.delete("/cache/clear")
async def clear_cache(
        document_id: Optional[str] = None,
        current_user: User = Depends(get_current_user)
):
    """æ¸…ç©ºç¼“å­˜"""
    if current_user.role != "admin":
        raise HTTPException(status_code=403, detail="Admin only")

    cache = get_semantic_cache()
    if not cache:
        return {"error": "Cache not initialized"}

    cache.clear_cache(document_id)
    return {"message": "Cache cleared"}