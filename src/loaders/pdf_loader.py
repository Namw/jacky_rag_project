"""
PDF æ–‡æ¡£åŠ è½½å™¨
èŒè´£ï¼šä»PDFæ–‡ä»¶ä¸­æå–æ–‡æœ¬ï¼Œå¹¶ä½¿ç”¨è¯­ä¹‰åˆ†å—å™¨å¤„ç†
"""

from pathlib import Path
from typing import List, Optional, Union
import fitz  # PyMuPDF
from langchain_core.documents import Document
from src.loaders.base_loader import BaseDocumentLoader
from src.processors.semantic_chunker import SemanticChunker
from src.processors.semantic_chunker import create_semantic_chunker

class PDFLoader(BaseDocumentLoader):
    """
    PDF æ–‡æ¡£åŠ è½½å™¨

    å¤„ç†æµç¨‹ï¼š
    1. æ‰“å¼€PDFæ–‡ä»¶
    2. é€é¡µæå–æ–‡æœ¬
    3. ä½¿ç”¨ SemanticChunker å¯¹æ¯é¡µæ–‡æœ¬è¿›è¡Œè¯­ä¹‰åˆ†å—
    4. ä¸ºæ¯ä¸ªåˆ†å—æ·»åŠ è¯¦ç»†å…ƒæ•°æ®ï¼ˆæ–‡ä»¶åã€é¡µç ã€è·¯å¾„ç­‰ï¼‰
    5. è¿”å› LangChain Document åˆ—è¡¨
    """

    def __init__(self, chunker: SemanticChunker, verbose: bool = True):
        """
        åˆå§‹åŒ– PDF åŠ è½½å™¨

        :param chunker: SemanticChunker å®ä¾‹
        :param verbose: æ˜¯å¦æ‰“å°å¤„ç†ä¿¡æ¯
        """
        super().__init__(verbose=verbose)
        self.chunker = chunker

    def load(self, file_path: Union[str, Path]) -> List[Document]:
        """
        åŠ è½½å•ä¸ª PDF æ–‡ä»¶

        :param file_path: PDF æ–‡ä»¶è·¯å¾„
        :return: Document åˆ—è¡¨
        """
        file_path = Path(file_path)

        # æ–‡ä»¶æ£€æŸ¥
        if not file_path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

        if file_path.suffix.lower() != '.pdf':
            raise ValueError(f"ä¸æ˜¯PDFæ–‡ä»¶: {file_path}")

        if self.verbose:
            print(f"ğŸ“„ æ­£åœ¨åŠ è½½: {file_path.name}")

        # æå–æ–‡æœ¬
        pages_data = self._extract_text(file_path)

        if not pages_data:
            if self.verbose:
                print(f"âš ï¸  {file_path.name} æ²¡æœ‰æå–åˆ°æ–‡æœ¬å†…å®¹")
            return []

        # åˆ†å—å¤„ç†
        documents = self._process_pages(pages_data, file_path)

        if self.verbose:
            print(f"âœ… {file_path.name}: å…± {len(pages_data)} é¡µï¼Œç”Ÿæˆ {len(documents)} ä¸ªåˆ†å—")

        return documents

    def _get_file_pattern(self) -> str:
        """
        è¿”å› PDF åŠ è½½å™¨æ”¯æŒçš„æ–‡ä»¶æ¨¡å¼

        :return: æ–‡ä»¶æ¨¡å¼
        """
        return "*.pdf"

    def _extract_text(self, file_path: Path) -> List[dict]:
        """
        ä» PDF ä¸­æå–æ–‡æœ¬ï¼ˆæŒ‰é¡µï¼‰

        :param file_path: PDF æ–‡ä»¶è·¯å¾„
        :return: é¡µé¢æ•°æ®åˆ—è¡¨ï¼Œæ ¼å¼: [{"page": 1, "text": "..."}, ...]
        """
        pages_data = []

        try:
            doc = fitz.open(str(file_path))

            for page_num, page in enumerate(doc):
                text = page.get_text("text").strip()

                # åªä¿ç•™éç©ºé¡µ
                if text:
                    pages_data.append({
                        "page": page_num + 1,  # é¡µç ä»1å¼€å§‹
                        "text": text
                    })

            doc.close()

        except Exception as e:
            raise RuntimeError(f"PDFè§£æå¤±è´¥: {e}")

        return pages_data

    def _process_pages(
            self,
            pages_data: List[dict],
            file_path: Path
    ) -> List[Document]:
        """
        å¤„ç†æ‰€æœ‰é¡µé¢ï¼Œç”Ÿæˆ Document åˆ—è¡¨

        :param pages_data: é¡µé¢æ•°æ®åˆ—è¡¨
        :param file_path: PDF æ–‡ä»¶è·¯å¾„
        :return: Document åˆ—è¡¨
        """
        all_documents = []

        for page_info in pages_data:
            page_num = page_info["page"]
            text = page_info["text"]

            # ä½¿ç”¨ SemanticChunker å¯¹æ¯ä¸€é¡µè¿›è¡Œåˆ†å—
            metadata = {
                "source": file_path.name,
                "file_path": str(file_path.absolute()),
                "page": page_num,
                "doc_type": "pdf"
            }

            page_documents = self.chunker.process_text(text, metadata)

            # ä¸ºæ¯ä¸ªåˆ†å—æ·»åŠ é¡µå†…ç¼–å·
            for i, doc in enumerate(page_documents):
                doc.metadata["page_chunk_id"] = i
                doc.metadata["page_chunk_total"] = len(page_documents)

            all_documents.extend(page_documents)

        return all_documents

    def get_pdf_info(self, file_path: Union[str, Path]) -> dict:
        """
        è·å– PDF æ–‡ä»¶çš„åŸºæœ¬ä¿¡æ¯ï¼ˆä¸è¿›è¡Œåˆ†å—ï¼‰

        :param file_path: PDF æ–‡ä»¶è·¯å¾„
        :return: PDF ä¿¡æ¯å­—å…¸
        """
        file_path = Path(file_path)

        if not file_path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

        doc = fitz.open(str(file_path))

        # ç»Ÿè®¡æ–‡æœ¬é¡µæ•°å’Œå­—ç¬¦æ•°
        text_pages = 0
        total_chars = 0

        for page in doc:
            text = page.get_text("text").strip()
            if text:
                text_pages += 1
                total_chars += len(text)

        info = {
            "filename": file_path.name,
            "file_path": str(file_path.absolute()),
            "file_size_mb": file_path.stat().st_size / (1024 * 1024),
            "total_pages": len(doc),
            "text_pages": text_pages,
            "empty_pages": len(doc) - text_pages,
            "total_chars": total_chars,
            "avg_chars_per_page": total_chars / text_pages if text_pages > 0 else 0,
            "metadata": doc.metadata
        }

        doc.close()

        return info


# ============ å·¥å‚å‡½æ•° ============
def create_pdf_loader(
        embedding_model,
        chunk_size: int = 300,
        chunk_overlap: float = 0.1,
        base_threshold: float = 0.8,
        dynamic_threshold: bool = True,
        window_size: int = 2,
        verbose: bool = True
) -> PDFLoader:
    """
    å¿«é€Ÿåˆ›å»º PDFLoader å®ä¾‹

    :param embedding_model: HuggingFace Embedding æ¨¡å‹
    :param chunk_size: åˆ†å—å¤§å°
    :param chunk_overlap: é‡å æ¯”ä¾‹
    :param base_threshold: åŸºç¡€ç›¸ä¼¼åº¦é˜ˆå€¼
    :param dynamic_threshold: æ˜¯å¦åŠ¨æ€è°ƒæ•´é˜ˆå€¼
    :param window_size: æ»‘åŠ¨çª—å£å¤§å°
    :param verbose: æ˜¯å¦æ‰“å°å¤„ç†ä¿¡æ¯
    :return: PDFLoader å®ä¾‹
    """

    chunker = create_semantic_chunker(
        embedding_model=embedding_model,
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        base_threshold=base_threshold,
        dynamic_threshold=dynamic_threshold,
        window_size=window_size,
        merge_separator=""  # ä¸­æ–‡é»˜è®¤æ— åˆ†éš”ç¬¦
    )

    return PDFLoader(chunker, verbose=verbose)