# src/services/semantic_cache.py
"""
è¯­ä¹‰æŸ¥è¯¢ç¼“å­˜ - åŸºäº query embedding çš„ç›¸ä¼¼åº¦åŒ¹é…
"""

import json
import redis
import numpy as np
from typing import List, Tuple, Optional, Dict, Any
from datetime import timedelta
from langchain_core.documents import Document

class SemanticQueryCache:
    """
    åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„æŸ¥è¯¢ç¼“å­˜

    ç¼“å­˜ç­–ç•¥ï¼š
    1. ä½¿ç”¨ query embedding è¿›è¡Œè¯­ä¹‰åŒ¹é…
    2. Rerank=True çš„ç¼“å­˜å¯ä»¥æœåŠ¡æ‰€æœ‰è¯·æ±‚
    3. Rerank=False çš„ç¼“å­˜åªèƒ½æœåŠ¡ Rerank=False çš„è¯·æ±‚
    4. TTL 24å°æ—¶
    """

    def __init__(
            self,
            redis_client: redis.Redis,
            embedding_model,
            similarity_threshold: float = 0.95,
            ttl_hours: int = 24
    ):
        """
        :param redis_client: Redis å®¢æˆ·ç«¯
        :param embedding_model: Embedding æ¨¡å‹ï¼ˆé˜¿é‡Œäº‘ DashScopeï¼‰
        :param similarity_threshold: è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼
        :param ttl_hours: ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆå°æ—¶ï¼‰
        """
        self.redis = redis_client
        self.embedding_model = embedding_model
        self.similarity_threshold = similarity_threshold
        self.ttl = timedelta(hours=ttl_hours)

        # Redis key å‰ç¼€
        self.QUERY_INDEX_KEY = "semantic_cache:queries"  # å­˜å‚¨æ‰€æœ‰ query çš„ç´¢å¼•
        self.RESULT_KEY_PREFIX = "semantic_cache:result:"  # å­˜å‚¨ç»“æœ

    def _compute_query_embedding(self, query: str) -> np.ndarray:
        """è®¡ç®— query çš„ embedding"""
        try:
            # é˜¿é‡Œäº‘ DashScope çš„è°ƒç”¨æ–¹å¼
            embedding = self.embedding_model.embed_query(query)
            return np.array(embedding, dtype=np.float32)
        except Exception as e:
            print(f"âŒ Embedding è®¡ç®—å¤±è´¥: {e}")
            raise

    def _compute_similarity(self, emb1: np.ndarray, emb2: np.ndarray) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        dot_product = np.dot(emb1, emb2)
        norm1 = np.linalg.norm(emb1)
        norm2 = np.linalg.norm(emb2)

        if norm1 == 0 or norm2 == 0:
            return 0.0

        return float(dot_product / (norm1 * norm2))

    def _make_cache_key(
            self,
            collection_name: str,  # ğŸ‘ˆ æ”¹ä¸º collection_name
            top_k: int
    ) -> str:
        """
        ç”Ÿæˆç¼“å­˜é”®çš„ç»„æˆéƒ¨åˆ†

        collection_name å’Œ top_k ä¼šå½±å“ç»“æœï¼Œéœ€è¦éš”ç¦»ç¼“å­˜
        """
        return f"{collection_name}:{top_k}"

    def get(
            self,
            query: str,
            collection_name: str,  # ğŸ‘ˆ æ”¹ä¸º collection_name
            top_k: int,
            use_rerank: bool
    ) -> Optional[List[Tuple[Document, float]]]:
        """
        ä»ç¼“å­˜ä¸­è·å–ç»“æœ

        :return: å¦‚æœå‘½ä¸­è¿”å› [(Document, score), ...]ï¼Œå¦åˆ™è¿”å› None
        """
        try:
            # 1. è®¡ç®—å½“å‰ query çš„ embedding
            query_embedding = self._compute_query_embedding(query)

            # 2. ç”Ÿæˆç¼“å­˜èŒƒå›´ key
            cache_scope = self._make_cache_key(collection_name, top_k)

            # 3. è·å–è¯¥èŒƒå›´ä¸‹çš„æ‰€æœ‰å·²ç¼“å­˜ query
            index_key = f"{self.QUERY_INDEX_KEY}:{cache_scope}"
            cached_queries_data = self.redis.hgetall(index_key)

            if not cached_queries_data:
                # print(f"ğŸ” ç¼“å­˜æœªå‘½ä¸­: è¯¥èŒƒå›´æ— ç¼“å­˜ (scope={cache_scope})")
                return None

            # 4. éå†æ‰¾åˆ°æœ€ç›¸ä¼¼çš„ query
            best_match = None
            best_similarity = 0.0

            for query_hash, query_info_json in cached_queries_data.items():
                query_info = json.loads(query_info_json)
                cached_embedding = np.array(query_info['embedding'], dtype=np.float32)

                # è®¡ç®—ç›¸ä¼¼åº¦
                similarity = self._compute_similarity(query_embedding, cached_embedding)

                if similarity > best_similarity:
                    best_similarity = similarity
                    best_match = query_info

            # 5. åˆ¤æ–­æ˜¯å¦è¾¾åˆ°é˜ˆå€¼
            if best_similarity < self.similarity_threshold:
                # print(f"ğŸ” ç¼“å­˜æœªå‘½ä¸­: ç›¸ä¼¼åº¦ä¸è¶³ (max={best_similarity:.3f}, threshold={self.similarity_threshold})")
                return None

            # 6. æ£€æŸ¥ Rerank å…¼å®¹æ€§
            cached_rerank = best_match['use_rerank']

            if not cached_rerank and use_rerank:
                # ç¼“å­˜æ˜¯æ™®é€šç»“æœï¼Œä½†è¯·æ±‚è¦ rerank ç»“æœ â†’ ä¸èƒ½ç”¨
                # print(f"ğŸ” ç¼“å­˜ä¸å¯ç”¨: éœ€è¦ rerank ä½†ç¼“å­˜ä¸ºæ™®é€šç»“æœ (similarity={best_similarity:.3f})")
                return None

            # 7. ä» Redis è·å–ç»“æœ
            result_key = best_match['result_key']
            cached_result_json = self.redis.get(result_key)

            if not cached_result_json:
                # print(f"âš ï¸ ç¼“å­˜è¿‡æœŸ: ç»“æœå·²å¤±æ•ˆ")
                # æ¸…ç†ç´¢å¼•
                self.redis.hdel(index_key, best_match['query_hash'])
                return None

            # 8. ååºåˆ—åŒ–ç»“æœ
            cached_data = json.loads(cached_result_json)
            results = self._deserialize_results(cached_data['results'])

            print(f"âœ… ç¼“å­˜å‘½ä¸­! collection={collection_name}, similarity={best_similarity:.3f}")

            return results

        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜è·å–å¼‚å¸¸: {e}")
            return None

    def set(
            self,
            query: str,
            collection_name: str,  # ğŸ‘ˆ æ”¹ä¸º collection_name
            top_k: int,
            use_rerank: bool,
            results: List[Tuple[Document, float]]
    ):
        """
        å°†ç»“æœå­˜å…¥ç¼“å­˜
        """
        try:
            # 1. è®¡ç®— query embedding
            query_embedding = self._compute_query_embedding(query)

            # 2. ç”Ÿæˆå”¯ä¸€æ ‡è¯†
            cache_scope = self._make_cache_key(collection_name, top_k)
            query_hash = self._hash_embedding(query_embedding)

            # 3. åºåˆ—åŒ–ç»“æœ
            serialized_results = self._serialize_results(results)

            # 4. å­˜å‚¨ç»“æœæ•°æ®ï¼ˆå¸¦ TTLï¼‰
            result_key = f"{self.RESULT_KEY_PREFIX}{cache_scope}:{query_hash}"
            result_data = {
                'results': serialized_results,
                'query_text': query,
                'use_rerank': use_rerank,
                'timestamp': self._get_timestamp()
            }
            self.redis.setex(
                result_key,
                self.ttl,
                json.dumps(result_data, ensure_ascii=False)
            )

            # 5. æ›´æ–°ç´¢å¼•ï¼ˆQuery â†’ Result çš„æ˜ å°„ï¼‰
            index_key = f"{self.QUERY_INDEX_KEY}:{cache_scope}"
            query_info = {
                'embedding': query_embedding.tolist(),
                'query_text': query,
                'use_rerank': use_rerank,
                'query_hash': query_hash,
                'result_key': result_key
            }
            self.redis.hset(
                index_key,
                query_hash,
                json.dumps(query_info, ensure_ascii=False)
            )
            # ç´¢å¼•ä¹Ÿè®¾ç½® TTLï¼ˆç¨é•¿ä¸€ç‚¹ï¼Œé¿å…æå‰è¿‡æœŸï¼‰
            self.redis.expire(index_key, self.ttl + timedelta(hours=1))

            # print(f"ğŸ’¾ ç¼“å­˜å·²ä¿å­˜: collection={collection_name}, query='{query[:30]}...', results={len(results)}")

        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜å­˜å‚¨å¼‚å¸¸: {e}")

    def _serialize_results(
            self,
            results: List[Tuple[Document, float]]
    ) -> List[Dict[str, Any]]:
        """åºåˆ—åŒ– Document å¯¹è±¡"""
        serialized = []
        for doc, score in results:
            serialized.append({
                'page_content': doc.page_content,
                'metadata': doc.metadata,
                'score': float(score)
            })
        return serialized

    def _deserialize_results(
            self,
            serialized: List[Dict[str, Any]]
    ) -> List[Tuple[Document, float]]:
        """ååºåˆ—åŒ–ä¸º Document å¯¹è±¡"""
        results = []
        for item in serialized:
            doc = Document(
                page_content=item['page_content'],
                metadata=item['metadata']
            )
            results.append((doc, item['score']))
        return results

    def _hash_embedding(self, embedding: np.ndarray) -> str:
        """å°† embedding è½¬æ¢ä¸ºå”¯ä¸€å“ˆå¸Œï¼ˆç”¨ä½œ Redis keyï¼‰"""
        import hashlib
        # å–å‰å‡ ä¸ªç»´åº¦ç”Ÿæˆå“ˆå¸Œï¼ˆå‡å°‘è®¡ç®—é‡ï¼‰
        truncated = embedding[:100].tobytes()
        return hashlib.md5(truncated).hexdigest()

    def _get_timestamp(self) -> str:
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        from datetime import datetime
        return datetime.now().isoformat()

    def clear_cache(self, collection_name: Optional[str] = None):  # ğŸ‘ˆ æ”¹ä¸º collection_name
        """æ¸…ç©ºç¼“å­˜ï¼ˆæ–‡æ¡£æ›´æ–°æ—¶ä½¿ç”¨ï¼‰"""
        try:
            if collection_name:
                # æ¸…ç©ºç‰¹å®š collection çš„ç¼“å­˜
                pattern = f"{self.QUERY_INDEX_KEY}:{collection_name}:*"
            else:
                # æ¸…ç©ºæ‰€æœ‰ç¼“å­˜
                pattern = f"{self.QUERY_INDEX_KEY}:*"

            keys = self.redis.keys(pattern)
            if keys:
                self.redis.delete(*keys)
                print(f"ğŸ—‘ï¸ å·²æ¸…ç©º {len(keys)} ä¸ªç¼“å­˜ç´¢å¼• (collection={collection_name or 'all'})")
        except Exception as e:
            print(f"âš ï¸ æ¸…ç©ºç¼“å­˜å¤±è´¥: {e}")

    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        try:
            all_index_keys = self.redis.keys(f"{self.QUERY_INDEX_KEY}:*")

            total_queries = 0
            rerank_count = 0
            collections = set()

            for index_key in all_index_keys:
                # ä» index_key è§£æ collection åç§°
                # æ ¼å¼: semantic_cache:queries:collection_name:top_k
                key_parts = index_key.decode('utf-8').split(':')
                if len(key_parts) >= 3:
                    collections.add(key_parts[2])

                queries_data = self.redis.hgetall(index_key)
                total_queries += len(queries_data)

                for query_info_json in queries_data.values():
                    query_info = json.loads(query_info_json)
                    if query_info.get('use_rerank'):
                        rerank_count += 1

            return {
                'total_cached_queries': total_queries,
                'rerank_cached': rerank_count,
                'normal_cached': total_queries - rerank_count,
                'cache_scopes': len(all_index_keys),
                'collections_cached': list(collections)  # ğŸ‘ˆ æ–°å¢ï¼šæ˜¾ç¤ºå“ªäº› collection æœ‰ç¼“å­˜
            }
        except Exception as e:
            print(f"âš ï¸ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {}