# src/services/scheduler_service.py
"""
å®šæ—¶ä»»åŠ¡è°ƒåº¦æœåŠ¡ - é›†ä¸­ç®¡ç†æ‰€æœ‰å®šæ—¶ä»»åŠ¡
"""
import os
import shutil
from pathlib import Path
from datetime import datetime
from apscheduler.schedulers.background import BackgroundScheduler

# ==================== å…¨å±€å®ä¾‹ ====================
_scheduler = None


# ==================== å®šæ—¶ä»»åŠ¡å‡½æ•° ====================

def _reset_limits_daily():
    """å®šæ—¶ä»»åŠ¡ï¼šé‡ç½®é™é¢åˆ°é»˜è®¤å€¼"""
    try:
        from src.services.usage_limiter import get_usage_limiter

        limiter = get_usage_limiter()
        limiter.reset_limits_to_default()
        print(f"âœ… [å®šæ—¶ä»»åŠ¡] {datetime.now().isoformat()} - é™é¢å·²é‡ç½®åˆ°é»˜è®¤å€¼")

    except Exception as e:
        print(f"âŒ [å®šæ—¶ä»»åŠ¡] é™é¢é‡ç½®å¤±è´¥: {e}")


def _clear_semantic_cache_daily():
    """å®šæ—¶ä»»åŠ¡ï¼šæ¸…ç†è¯­ä¹‰æŸ¥è¯¢ç¼“å­˜"""
    try:
        from src.services.retrieval_service import get_semantic_cache

        cache = get_semantic_cache()
        if cache is None:
            print("âš ï¸  [å®šæ—¶ä»»åŠ¡] è¯­ä¹‰ç¼“å­˜æœªåˆå§‹åŒ–ï¼Œè·³è¿‡æ¸…ç†")
            return

        # æ¸…ç†æ‰€æœ‰ç¼“å­˜
        cache.clear_cache(collection_name=None)
        print(f"âœ… [å®šæ—¶ä»»åŠ¡] {datetime.now().isoformat()} - è¯­ä¹‰ç¼“å­˜å·²æ¸…ç†")

    except Exception as e:
        print(f"âŒ [å®šæ—¶ä»»åŠ¡] ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")


def _cleanup_upload_files_daily():
    """å®šæ—¶ä»»åŠ¡ï¼šæ¸…ç†è¿‡æœŸçš„ä¸Šä¼ æ–‡ä»¶ â­ï¸ æ–°å¢"""
    try:
        upload_dir = Path("data/uploads")

        if not upload_dir.exists():
            print("âš ï¸  [å®šæ—¶ä»»åŠ¡] ä¸Šä¼ æ–‡ä»¶ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡æ¸…ç†")
            return

        # è·å–ç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶
        files = list(upload_dir.glob("*"))

        if not files:
            print(f"âœ… [å®šæ—¶ä»»åŠ¡] {datetime.now().isoformat()} - ä¸Šä¼ æ–‡ä»¶ç›®å½•å·²ä¸ºç©ºï¼Œæ— éœ€æ¸…ç†")
            return

        # åˆ é™¤æ‰€æœ‰æ–‡ä»¶
        deleted_count = 0
        total_size = 0

        for file_path in files:
            try:
                if file_path.is_file():
                    file_size = file_path.stat().st_size
                    file_path.unlink()  # åˆ é™¤æ–‡ä»¶
                    deleted_count += 1
                    total_size += file_size
                elif file_path.is_dir():
                    shutil.rmtree(file_path)  # åˆ é™¤ç›®å½•
                    deleted_count += 1
            except Exception as e:
                print(f"âš ï¸  [å®šæ—¶ä»»åŠ¡] åˆ é™¤æ–‡ä»¶å¤±è´¥ {file_path}: {e}")

        # æ ¼å¼åŒ–æ–‡ä»¶å¤§å°
        size_mb = total_size / (1024 * 1024)

        print(f"âœ… [å®šæ—¶ä»»åŠ¡] {datetime.now().isoformat()} - ä¸Šä¼ æ–‡ä»¶å·²æ¸…ç†")
        print(f"   - å·²åˆ é™¤: {deleted_count} ä¸ªæ–‡ä»¶/ç›®å½•")
        print(f"   - é‡Šæ”¾ç©ºé—´: {size_mb:.2f} MB")

    except Exception as e:
        print(f"âŒ [å®šæ—¶ä»»åŠ¡] ä¸Šä¼ æ–‡ä»¶æ¸…ç†å¤±è´¥: {e}")


# ==================== è°ƒåº¦å™¨åˆå§‹åŒ– ====================

def init_scheduler():
    """åˆå§‹åŒ–å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨ â­ï¸ æ–°å¢"""
    global _scheduler

    try:
        _scheduler = BackgroundScheduler()

        # ä»ç¯å¢ƒå˜é‡è·å–é‡ç½®æ—¶é—´é…ç½®
        reset_hour = int(os.getenv("RESET_HOUR", "6"))
        reset_minute = int(os.getenv("RESET_MINUTE", "0"))

        # 1ï¸âƒ£ é™é¢é‡ç½®ä»»åŠ¡
        _scheduler.add_job(
            func=_reset_limits_daily,
            trigger="cron",
            hour=reset_hour,
            minute=reset_minute,
            id="reset_limits_daily",
            name="æ¯æ—¥é™é¢é‡ç½®ä»»åŠ¡",
            timezone="Asia/Shanghai"
        )

        # 2ï¸âƒ£ ç¼“å­˜æ¸…ç†ä»»åŠ¡
        _scheduler.add_job(
            func=_clear_semantic_cache_daily,
            trigger="cron",
            hour=reset_hour,
            minute=reset_minute,
            id="clear_cache_daily",
            name="æ¯æ—¥ç¼“å­˜æ¸…ç†ä»»åŠ¡",
            timezone="Asia/Shanghai"
        )

        # 3ï¸âƒ£ ä¸Šä¼ æ–‡ä»¶æ¸…ç†ä»»åŠ¡ â­ï¸ æ–°å¢
        _scheduler.add_job(
            func=_cleanup_upload_files_daily,
            trigger="cron",
            hour=reset_hour,
            minute=reset_minute,
            id="cleanup_upload_files_daily",
            name="æ¯æ—¥ä¸Šä¼ æ–‡ä»¶æ¸…ç†ä»»åŠ¡",
            timezone="Asia/Shanghai"
        )

        _scheduler.start()
        print(f"âœ… å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²å¯åŠ¨: æ¯å¤© {reset_hour:02d}:{reset_minute:02d} æ‰§è¡Œ")
        print(f"   ğŸ“‹ ä»»åŠ¡åˆ—è¡¨:")
        print(f"      1ï¸âƒ£  é™é¢é‡ç½®")
        print(f"      2ï¸âƒ£  è¯­ä¹‰ç¼“å­˜æ¸…ç†")
        print(f"      3ï¸âƒ£  ä¸Šä¼ æ–‡ä»¶æ¸…ç†")

    except Exception as e:
        print(f"âš ï¸  å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å¯åŠ¨å¤±è´¥: {e}")


def stop_scheduler():
    """åœæ­¢å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨ï¼ˆåº”ç”¨å…³é—­æ—¶è°ƒç”¨ï¼‰"""
    global _scheduler
    if _scheduler and _scheduler.running:
        _scheduler.shutdown()
        print("âœ… å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²åœæ­¢")


def get_scheduler():
    """è·å–è°ƒåº¦å™¨å®ä¾‹ï¼ˆç”¨äºæµ‹è¯•æˆ–ç›‘æ§ï¼‰"""
    return _scheduler


def get_scheduler_status():
    """è·å–è°ƒåº¦å™¨çŠ¶æ€ï¼ˆç”¨äºè°ƒè¯•ï¼‰"""
    if not _scheduler:
        return {
            "status": "not initialized",
            "jobs": []
        }

    if not _scheduler.running:
        return {
            "status": "stopped",
            "jobs": []
        }

    jobs_info = []
    for job in _scheduler.get_jobs():
        jobs_info.append({
            "id": job.id,
            "name": job.name,
            "next_run_time": str(job.next_run_time) if job.next_run_time else None
        })

    return {
        "status": "running",
        "jobs": jobs_info
    }
