# src/services/retrieval_service.py
"""
ç»Ÿä¸€çš„å¬å›æœåŠ¡
documents.pyï¼ˆæµ‹è¯•ï¼‰å’Œ chat.pyï¼ˆå®é™…ä½¿ç”¨ï¼‰éƒ½è°ƒç”¨è¿™ä¸ª
"""
import redis
from typing import List, Tuple, Optional
from langchain_core.documents import Document
from langchain_chroma import Chroma
from pathlib import Path
from src.services.semantic_cache import SemanticQueryCache
from models.model_factory_ebd import ModelFactoryEbd

CHROMA_PERMANENT_DIR = Path("data/vectorstore/permanent")  # æ­£å¼åº“

# ==================== å…¨å±€æ¨¡å‹å®ä¾‹ï¼ˆæ‡’åŠ è½½ï¼‰====================
print(f"ğŸ”§ å½“å‰æ¨¡å‹æä¾›å•†: {ModelFactoryEbd.get_provider()}")

def get_embedding_model():
    """è·å– Embedding æ¨¡å‹"""
    return ModelFactoryEbd.get_embedding_model()

def get_reranker_model():
    """è·å– Reranker æ¨¡å‹"""
    return ModelFactoryEbd.get_reranker_model()

_semantic_cache: Optional[SemanticQueryCache] = None

def init_semantic_cache(
        redis_client: redis.Redis,
        similarity_threshold: float = 0.95,
        ttl_hours: int = 24
    ):
    """åˆå§‹åŒ–è¯­ä¹‰ç¼“å­˜ï¼ˆéœ€è¦åœ¨åº”ç”¨å¯åŠ¨æ—¶è°ƒç”¨ï¼‰"""
    global _semantic_cache
    embedding_model = get_embedding_model()
    _semantic_cache = SemanticQueryCache(
        redis_client=redis_client,
        embedding_model=embedding_model,
        similarity_threshold=similarity_threshold,  # å¯ä»¥è°ƒæ•´
        ttl_hours=ttl_hours
    )
    print("âœ… è¯­ä¹‰ç¼“å­˜åˆå§‹åŒ–æˆåŠŸ")

def get_semantic_cache() -> Optional[SemanticQueryCache]:
    """è·å–è¯­ä¹‰ç¼“å­˜å®ä¾‹"""
    return _semantic_cache

def retrieve_with_cache(
        vectorstore: Chroma,
        query: str,
        collection_name: str,  # ğŸ‘ˆ æ”¹ä¸º collection_nameï¼Œå¿…ä¼ 
        top_k: int = 5,
        use_rerank: bool = False,
        threshold: Optional[float] = None
) -> List[Tuple[Document, float]]:
    """
    å¸¦è¯­ä¹‰ç¼“å­˜çš„æ£€ç´¢å‡½æ•°

    :param vectorstore: Chroma å‘é‡åº“å®ä¾‹
    :param query: æŸ¥è¯¢æ–‡æœ¬
    :param collection_name: Collection åç§°ï¼ˆç”¨äºç¼“å­˜éš”ç¦»ï¼‰
    :param top_k: è¿”å› top-k ç»“æœ
    :param use_rerank: æ˜¯å¦å¯ç”¨ rerank
    :param threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
    :return: [(Document, score), ...]
    """
    cache = get_semantic_cache()

    # 1. å°è¯•ä»ç¼“å­˜è·å–
    if cache:
        cached_results = cache.get(
            query=query,
            collection_name=collection_name,
            top_k=top_k,
            use_rerank=use_rerank
        )

        if cached_results is not None:
            # åº”ç”¨ threshold è¿‡æ»¤ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if threshold is not None:
                cached_results = [
                    (doc, score) for doc, score in cached_results
                    if score >= threshold
                ]
            return cached_results

    # 2. ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡ŒçœŸå®æ£€ç´¢
    results = retrieve_with_rerank(
        vectorstore=vectorstore,
        query=query,
        top_k=top_k,
        use_rerank=use_rerank,
        threshold=threshold
    )

    # 3. å­˜å…¥ç¼“å­˜
    if cache and results:
        cache.set(
            query=query,
            collection_name=collection_name,
            top_k=top_k,
            use_rerank=use_rerank,
            results=results
        )

    return results

def retrieve_with_rerank(
        vectorstore: Chroma,
        query: str,
        top_k: int = 5,
        use_rerank: bool = False,
        threshold: Optional[float] = None
) -> List[Tuple[Document, float]]:
    """
    ç»Ÿä¸€çš„å¬å›å‡½æ•°

    :param vectorstore: Chroma å‘é‡åº“å®ä¾‹
    :param query: æŸ¥è¯¢æ–‡æœ¬
    :param top_k: è¿”å› top-k ç»“æœ
    :param use_rerank: æ˜¯å¦å¯ç”¨ rerank äºŒæ¬¡ç²¾æ’
    :param threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆå¯é€‰ï¼‰
    :return: [(Document, score), ...] åˆ—è¡¨
    """

    # 1. å¬å›ï¼ˆå¦‚æœå¯ç”¨ rerankï¼Œå¤šå¬å›ä¸€äº›ç”¨äºç²¾æ’ï¼‰
    initial_k = top_k * 3 if use_rerank else top_k

    results_with_scores = vectorstore.similarity_search_with_score(
        query,
        k=initial_k
    )
    # print(f"æ£€ç´¢çš„æ–‡æœ¬åˆ†ç±»ï¼š{results_with_scores[0][0].metadata['category']}")
    # 2. è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼ï¼ˆdistance â†’ similarityï¼‰
    results = []
    for doc, distance in results_with_scores:
        similarity = 1 - distance  # Chroma è¿”å›çš„æ˜¯ distance

        # é˜ˆå€¼è¿‡æ»¤
        if threshold is not None and similarity < threshold:
            continue

        results.append((doc, similarity))

    # 3. Rerank äºŒæ¬¡ç²¾æ’ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if use_rerank and len(results) > 0:
        results = _rerank_results(query, results, top_k)

    # 4. è¿”å› top_k ä¸ªç»“æœ
    return results[:top_k]

def _rerank_results(
        query: str,
        results: List[Tuple[Document, float]],
        top_k: int
) -> List[Tuple[Document, float]]:
    """ä½¿ç”¨ Reranker è¿›è¡ŒäºŒæ¬¡ç²¾æ’ï¼ˆå…¼å®¹æœ¬åœ°å’Œé˜¿é‡Œäº‘æ¨¡å‹ï¼‰"""

    reranker_model = get_reranker_model()

    if reranker_model is None:
        print("âš ï¸ Reranker ä¸å¯ç”¨ï¼Œè¿”å›åŸå§‹ç»“æœ")
        return results[:top_k]

    try:
        # å‡†å¤‡ query-document å¯¹
        pairs = [[query, doc.page_content] for doc, _ in results]

        # è®¡ç®— rerank åˆ†æ•°ï¼ˆç»Ÿä¸€æ¥å£ï¼‰
        rerank_scores = reranker_model.predict(pairs)

        # æ›´æ–°åˆ†æ•°å¹¶é‡æ–°æ’åº
        reranked = []
        for i, (doc, _) in enumerate(results):
            reranked.append((doc, float(rerank_scores[i])))

        # æŒ‰ rerank åˆ†æ•°é™åºæ’åº
        reranked.sort(key=lambda x: x[1], reverse=True)

        return reranked[:top_k]

    except Exception as e:
        print(f"âš ï¸ Rerank å¤±è´¥: {e}")
        return results[:top_k]

embedding_model = get_embedding_model()
