"""
RAG Pipeline ä¸»æµç¨‹
æ£€ç´¢ â†’ Promptç»„è£… â†’ LLMç”Ÿæˆ â†’ ç­”æ¡ˆè¿”å›
"""

from typing import List, Optional, Dict, Any
from langchain_core.documents import Document
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI

from src.vectorstore.chroma_store import VectorStoreManager
from models.model_factory import ModelFactory
from models.model_paths import get_models_cache_dir
from config.model_config import ModelProvider


class RAGPipeline:
    """
    RAG ä¸»æµç¨‹ç®¡ç†å™¨

    åŠŸèƒ½ï¼š
    1. å‘é‡æ£€ç´¢ç›¸å…³æ–‡æ¡£
    2. ç»„è£… Prompt
    3. è°ƒç”¨ LLM ç”Ÿæˆç­”æ¡ˆ
    4. è¿”å›ç­”æ¡ˆ + æ¥æº
    """

    def __init__(
            self,
            vectorstore_manager: VectorStoreManager,
            model_provider: ModelProvider = ModelProvider.DEEPSEEK,
            top_k: int = 5,
            temperature: float = 0.7,
            verbose: bool = True
    ):
        """
        åˆå§‹åŒ– RAG Pipeline

        :param vectorstore_manager: å‘é‡æ•°æ®åº“ç®¡ç†å™¨
        :param model_provider: LLM æä¾›å•†
        :param top_k: æ£€ç´¢æ–‡æ¡£æ•°é‡
        :param temperature: LLM æ¸©åº¦
        :param verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        """
        self.vectorstore = vectorstore_manager
        self.model_provider = model_provider
        self.top_k = top_k
        self.temperature = temperature
        self.verbose = verbose

        # åˆå§‹åŒ– LLM
        self.llm = self._init_llm()

    def _init_llm(self) -> ChatOpenAI:
        """åˆå§‹åŒ– LLM æ¨¡å‹"""
        if self.verbose:
            print(f"\nâ³ åˆå§‹åŒ– LLM: {self.model_provider.value}")

        llm = ModelFactory.create_model(
            provider=self.model_provider,
            temperature=self.temperature
        )

        if self.verbose:
            print(f"âœ… LLM åˆå§‹åŒ–å®Œæˆ\n")

        return llm

    def retrieve_documents(
            self,
            query: str,
            k: Optional[int] = None,
            filter_dict: Optional[Dict[str, Any]] = None
    ) -> List[tuple[Document, float]]:
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£

        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param k: æ£€ç´¢æ•°é‡ï¼ˆå¦‚æœä¸æŒ‡å®šåˆ™ä½¿ç”¨åˆå§‹åŒ–æ—¶çš„ top_kï¼‰
        :param filter_dict: å…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶
        :return: (Document, score) åˆ—è¡¨
        """
        k = k or self.top_k

        if self.verbose:
            print(f"ğŸ” æ£€ç´¢ç›¸å…³æ–‡æ¡£ (top_k={k})...")

        results = self.vectorstore.search_with_score(
            query=query,
            k=k,
            filter_dict=filter_dict
        )

        if self.verbose:
            print(f"   æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£\n")

        return results

    def build_prompt(
            self,
            query: str,
            documents: List[tuple[Document, float]],
            system_prompt: Optional[str] = None
    ) -> tuple[str, str]:
        """
        æ„å»º Prompt

        :param query: ç”¨æˆ·é—®é¢˜
        :param documents: æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
        :param system_prompt: è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯
        :return: (system_message, user_message)
        """
        # é»˜è®¤ç³»ç»Ÿæç¤ºè¯
        if system_prompt is None:
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åŠ©æ‰‹ã€‚

ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®æä¾›çš„å‚è€ƒèµ„æ–™å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

è¦æ±‚ï¼š
1. ä»”ç»†é˜…è¯»å‚è€ƒèµ„æ–™ï¼ŒåŸºäºèµ„æ–™å†…å®¹å›ç­”
2. å¦‚æœèµ„æ–™ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯šå®åœ°å‘Šè¯‰ç”¨æˆ·"æ ¹æ®æä¾›çš„èµ„æ–™ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜"
3. å›ç­”è¦å‡†ç¡®ã€ç®€æ´ã€æœ‰æ¡ç†
4. å¯ä»¥é€‚å½“å¼•ç”¨èµ„æ–™ä¸­çš„å…³é”®ä¿¡æ¯
5. ä¸è¦ç¼–é€ èµ„æ–™ä¸­æ²¡æœ‰çš„å†…å®¹"""

        # ç»„è£…å‚è€ƒèµ„æ–™
        context_parts = []
        for i, (doc, score) in enumerate(documents, 1):
            source = doc.metadata.get('source', 'Unknown')
            page = doc.metadata.get('page', 'N/A')
            content = doc.page_content

            context_parts.append(
                f"ã€å‚è€ƒèµ„æ–™ {i}ã€‘\n"
                f"æ¥æº: {source} (ç¬¬{page}é¡µ)\n"
                f"å†…å®¹: {content}\n"
            )

        context = "\n".join(context_parts)

        # ç”¨æˆ·æ¶ˆæ¯
        user_message = f"""å‚è€ƒèµ„æ–™ï¼š
{context}

---

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·åŸºäºä¸Šè¿°å‚è€ƒèµ„æ–™å›ç­”é—®é¢˜ã€‚"""

        return system_prompt, user_message

    def generate_answer(
            self,
            query: str,
            documents: List[tuple[Document, float]],
            system_prompt: Optional[str] = None
    ) -> str:
        """
        ç”Ÿæˆç­”æ¡ˆ

        :param query: ç”¨æˆ·é—®é¢˜
        :param documents: æ£€ç´¢åˆ°çš„æ–‡æ¡£
        :param system_prompt: è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯
        :return: LLM ç”Ÿæˆçš„ç­”æ¡ˆ
        """
        # æ„å»º Prompt
        sys_msg, user_msg = self.build_prompt(query, documents, system_prompt)

        if self.verbose:
            print(f"ğŸ¤– è°ƒç”¨ LLM ç”Ÿæˆç­”æ¡ˆ...")

        # è°ƒç”¨ LLM
        messages = [
            SystemMessage(content=sys_msg),
            HumanMessage(content=user_msg)
        ]

        try:
            response = self.llm.invoke(messages)
            answer = response.content

            if self.verbose:
                print(f"âœ… ç­”æ¡ˆç”Ÿæˆå®Œæˆ\n")

            return answer

        except Exception as e:
            error_msg = f"LLM è°ƒç”¨å¤±è´¥: {str(e)}"
            if self.verbose:
                print(f"âŒ {error_msg}\n")
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°é”™è¯¯ï¼š{error_msg}"

    def query(
            self,
            question: str,
            top_k: Optional[int] = None,
            filter_dict: Optional[Dict[str, Any]] = None,
            return_sources: bool = True
    ) -> Dict[str, Any]:
        """
        å®Œæ•´çš„ RAG æŸ¥è¯¢æµç¨‹

        :param question: ç”¨æˆ·é—®é¢˜
        :param top_k: æ£€ç´¢æ–‡æ¡£æ•°é‡
        :param filter_dict: å…ƒæ•°æ®è¿‡æ»¤
        :param return_sources: æ˜¯å¦è¿”å›æ¥æºä¿¡æ¯
        :return: åŒ…å«ç­”æ¡ˆå’Œæ¥æºçš„å­—å…¸
        """
        if self.verbose:
            print(f"\n{'=' * 70}")
            print(f"RAG æŸ¥è¯¢")
            print(f"{'=' * 70}")
            print(f"é—®é¢˜: {question}\n")

        # 1. æ£€ç´¢æ–‡æ¡£
        documents = self.retrieve_documents(question, k=top_k, filter_dict=filter_dict)

        if not documents:
            return {
                "answer": "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„å‚è€ƒèµ„æ–™ã€‚",
                "sources": [],
                "question": question
            }

        # 2. ç”Ÿæˆç­”æ¡ˆ
        answer = self.generate_answer(question, documents)

        # 3. æ•´ç†è¿”å›ç»“æœ
        result = {
            "answer": answer,
            "question": question
        }

        if return_sources:
            sources = []
            for doc, score in documents:
                sources.append({
                    "source": doc.metadata.get('source', 'Unknown'),
                    "page": doc.metadata.get('page', 'N/A'),
                    "content": doc.page_content[:200] + "...",
                    "score": float(score)
                })
            result["sources"] = sources

        if self.verbose:
            print(f"{'=' * 70}\n")

        return result

    def switch_model(self, provider: ModelProvider):
        """
        åˆ‡æ¢ LLM æä¾›å•†

        :param provider: æ–°çš„æ¨¡å‹æä¾›å•†
        """
        self.model_provider = provider
        self.llm = self._init_llm()

    def print_answer(self, result: Dict[str, Any]):
        """
        æ ¼å¼åŒ–æ‰“å°ç­”æ¡ˆ

        :param result: query() è¿”å›çš„ç»“æœ
        """
        print(f"\n{'=' * 70}")
        print(f"é—®é¢˜: {result['question']}")
        print(f"{'=' * 70}\n")

        print(f"ğŸ“ ç­”æ¡ˆï¼š")
        print(result['answer'])

        if 'sources' in result and result['sources']:
            print(f"\n{'â”€' * 70}")
            print(f"ğŸ“š å‚è€ƒæ¥æº:")
            for i, source in enumerate(result['sources'], 1):
                print(f"\n  [{i}] {source['source']} (ç¬¬{source['page']}é¡µ)")
                print(f"      ç›¸ä¼¼åº¦: {source['score']:.4f}")
                print(f"      å†…å®¹: {source['content']}")

        print(f"\n{'=' * 70}\n")


# ============ å·¥å‚å‡½æ•° ============
def create_rag_pipeline(
        vectorstore_manager: VectorStoreManager,
        model_provider: ModelProvider = ModelProvider.DEEPSEEK,
        top_k: int = 5,
        temperature: float = 0.7,
        verbose: bool = True
) -> RAGPipeline:
    """
    å¿«é€Ÿåˆ›å»º RAG Pipeline

    :param vectorstore_manager: å‘é‡æ•°æ®åº“ç®¡ç†å™¨
    :param model_provider: LLM æä¾›å•†
    :param top_k: æ£€ç´¢æ–‡æ¡£æ•°é‡
    :param temperature: LLM æ¸©åº¦
    :param verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    :return: RAGPipeline å®ä¾‹
    """
    return RAGPipeline(
        vectorstore_manager=vectorstore_manager,
        model_provider=model_provider,
        top_k=top_k,
        temperature=temperature,
        verbose=verbose
    )


# ============ ä½¿ç”¨ç¤ºä¾‹ ============
if __name__ == "__main__":
    from langchain_huggingface import HuggingFaceEmbeddings
    from src.vectorstore.chroma_store import create_vectorstore_manager
    # åŠ è½½ç¯å¢ƒå˜é‡
    from dotenv import load_dotenv
    load_dotenv()

    # 1. åˆå§‹åŒ– embedding
    print("åˆå§‹åŒ– Embedding...")
    embedding = HuggingFaceEmbeddings(
        model_name="BAAI/bge-large-zh-v1.5",
        cache_folder=get_models_cache_dir(),
        model_kwargs={
            "device": "cpu",
            "local_files_only": True
        }
    )

    # 2. åˆå§‹åŒ–å‘é‡æ•°æ®åº“
    print("åˆå§‹åŒ–å‘é‡æ•°æ®åº“...")
    vectorstore = create_vectorstore_manager(
        embedding_model=embedding,
        persist_directory="../data/vectorstore"
    )

    # 3. åˆ›å»º RAG Pipeline
    print("åˆ›å»º RAG Pipeline...")
    rag = create_rag_pipeline(
        vectorstore_manager=vectorstore,
        model_provider=ModelProvider.DEEPSEEK,
        top_k=3,
        verbose=True
    )

    # 4. æµ‹è¯•æŸ¥è¯¢
    test_questions = [
        "åŠ³åŠ¨åˆåŒçš„æœŸé™æ˜¯å¤šä¹…ï¼Ÿ",
        "å·¥ä½œåœ°ç‚¹åœ¨å“ªé‡Œï¼Ÿ",
        "æ±ªæ˜¥å…»çš„å·¥ä½œç»å†æœ‰å“ªäº›ï¼Ÿ"
    ]

    for question in test_questions:
        result = rag.query(question)
        rag.print_answer(result)
        input("\næŒ‰ Enter ç»§ç»­ä¸‹ä¸€ä¸ªé—®é¢˜...")

    # 5. æµ‹è¯•åˆ‡æ¢æ¨¡å‹
    print("\nåˆ‡æ¢åˆ° Qwen æ¨¡å‹...")
    rag.switch_model(ModelProvider.QWEN)

    result = rag.query("æ€»ç»“ä¸€ä¸‹åŠ³åŠ¨åˆåŒçš„ä¸»è¦å†…å®¹")
    rag.print_answer(result)