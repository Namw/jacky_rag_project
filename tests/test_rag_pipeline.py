"""
å®Œæ•´ RAG æµç¨‹æµ‹è¯•
PDFåŠ è½½ â†’ å‘é‡å­˜å‚¨ â†’ æ£€ç´¢ â†’ LLMç”Ÿæˆç­”æ¡ˆ
"""

import os
from dotenv import load_dotenv
from langchain_huggingface import HuggingFaceEmbeddings

from models.model_paths import get_models_cache_dir
from src.loaders.pdf_loader import create_pdf_loader
from src.vectorstore.chroma_store import create_vectorstore_manager
from src.rag_pipeline import create_rag_pipeline
from config.model_config import ModelProvider

os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")

def test_rag_query():
    """æµ‹è¯• RAG å®Œæ•´æŸ¥è¯¢æµç¨‹"""

    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()

    print("\n" + "ğŸš€ " + "=" * 66 + " ğŸš€")
    print("     å®Œæ•´ RAG æµç¨‹æµ‹è¯•")
    print("ğŸš€ " + "=" * 66 + " ğŸš€\n")

    # ========== æ­¥éª¤1: åˆå§‹åŒ–èµ„æº ==========
    print("=" * 70)
    print("æ­¥éª¤1: åˆå§‹åŒ–èµ„æº")
    print("=" * 70 + "\n")

    # 1.1 åŠ è½½ Embedding æ¨¡å‹
    print("â³ åŠ è½½ Embedding æ¨¡å‹...")
    embedding = HuggingFaceEmbeddings(
        model_name="BAAI/bge-large-zh-v1.5",
        cache_folder=get_models_cache_dir(),
        model_kwargs={
            "device": "cpu",
            "local_files_only": True
        }
    )
    print("âœ… Embedding æ¨¡å‹åŠ è½½å®Œæˆ")

    # 1.2 åˆå§‹åŒ–å‘é‡æ•°æ®åº“
    print("â³ åˆå§‹åŒ–å‘é‡æ•°æ®åº“...")
    vectorstore = create_vectorstore_manager(
        embedding_model=embedding,
        persist_directory="../data/vectorstore"
    )
    print("âœ… å‘é‡æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")

    # æŸ¥çœ‹æ•°æ®åº“çŠ¶æ€
    vectorstore.print_stats()

    # ========== æ­¥éª¤2: å¦‚æœæ•°æ®åº“ä¸ºç©ºï¼Œå…ˆåŠ è½½æ–‡æ¡£ ==========
    stats = vectorstore.get_stats()
    if stats.get('total_documents', 0) == 0:
        print("\n" + "=" * 70)
        print("æ­¥éª¤2: æ•°æ®åº“ä¸ºç©ºï¼ŒåŠ è½½æ–‡æ¡£...")
        print("=" * 70 + "\n")

        loader = create_pdf_loader(
            embedding_model=embedding,
            chunk_size=300,
            chunk_overlap=0.1,
            verbose=False
        )

        pdf_files = [
            "../data/documents/åŠ³åŠ¨åˆåŒ.pdf",
            "../data/documents/æ±ªæ˜¥å…»ç®€å†.pdf"
        ]

        existing_files = [f for f in pdf_files if os.path.exists(f)]

        if existing_files:
            documents = loader.load_batch(existing_files)
            vectorstore.add_documents(documents, verbose=True)
            print("\næ•°æ®åŠ è½½å®Œæˆï¼")
            vectorstore.print_stats()
        else:
            print("âš ï¸  æœªæ‰¾åˆ°PDFæ–‡ä»¶ï¼Œæ— æ³•è¿›è¡Œæµ‹è¯•")
            return

    # ========== æ­¥éª¤3: åˆ›å»º RAG Pipeline ==========
    print("\n" + "=" * 70)
    print("æ­¥éª¤3: åˆ›å»º RAG Pipeline")
    print("=" * 70 + "\n")

    rag = create_rag_pipeline(
        vectorstore_manager=vectorstore,
        model_provider=ModelProvider.DEEPSEEK,  # ä½¿ç”¨ DeepSeek
        top_k=3,
        temperature=0.7,
        verbose=True
    )

    # ========== æ­¥éª¤4: æµ‹è¯•æŸ¥è¯¢ ==========
    print("\n" + "=" * 70)
    print("æ­¥éª¤4: æµ‹è¯• RAG æŸ¥è¯¢")
    print("=" * 70 + "\n")

    test_questions = [
        "å·¥ä½œåœ°ç‚¹åœ¨å“ªé‡Œï¼Ÿ",
        "åŠ³åŠ¨åˆåŒçš„æœŸé™æ˜¯å¤šä¹…ï¼Ÿ",
        "å·¥èµ„å¾…é‡æ˜¯æ€æ ·çš„ï¼Ÿ"
    ]

    for i, question in enumerate(test_questions, 1):
        print(f"\n{'â”€' * 70}")
        print(f"æŸ¥è¯¢ {i}/{len(test_questions)}")
        print(f"{'â”€' * 70}\n")

        result = rag.query(question)
        rag.print_answer(result)

        if i < len(test_questions):
            input("æŒ‰ Enter ç»§ç»­ä¸‹ä¸€ä¸ªæŸ¥è¯¢...")

    # ========== æ­¥éª¤5: æµ‹è¯•å¸¦è¿‡æ»¤çš„æŸ¥è¯¢ ==========
    print("\n" + "=" * 70)
    print("æ­¥éª¤5: æµ‹è¯•å…ƒæ•°æ®è¿‡æ»¤æŸ¥è¯¢")
    print("=" * 70 + "\n")

    # åªåœ¨"åŠ³åŠ¨åˆåŒ.pdf"ä¸­æ£€ç´¢
    question = "è¿™ä»½åˆåŒçš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ"
    result = rag.query(
        question,
        filter_dict={"source": "åŠ³åŠ¨åˆåŒ.pdf"}
    )
    rag.print_answer(result)

    print("\n" + "âœ… " + "=" * 66 + " âœ…")
    print("     æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print("âœ… " + "=" * 66 + " âœ…\n")


def test_model_switching():
    """æµ‹è¯•æ¨¡å‹åˆ‡æ¢åŠŸèƒ½"""

    load_dotenv()

    print("\n" + "=" * 70)
    print("æµ‹è¯•ï¼šæ¨¡å‹åˆ‡æ¢")
    print("=" * 70 + "\n")

    # åˆå§‹åŒ–èµ„æº
    embedding = HuggingFaceEmbeddings(
        model_name="BAAI/bge-large-zh-v1.5",
        cache_folder=get_models_cache_dir(),
        model_kwargs={"device": "cpu", "local_files_only": True}
    )

    vectorstore = create_vectorstore_manager(
        embedding_model=embedding,
        persist_directory="../data/vectorstore"
    )

    # åˆ›å»º RAG Pipelineï¼ˆé»˜è®¤ DeepSeekï¼‰
    rag = create_rag_pipeline(
        vectorstore_manager=vectorstore,
        model_provider=ModelProvider.DEEPSEEK,
        verbose=True
    )

    question = "æ€»ç»“ä¸€ä¸‹åŠ³åŠ¨åˆåŒçš„ä¸»è¦å†…å®¹"

    # æµ‹è¯• DeepSeek
    print("\nã€ä½¿ç”¨ DeepSeekã€‘")
    result = rag.query(question, top_k=3)
    rag.print_answer(result)

    # åˆ‡æ¢åˆ° Qwen
    print("\n" + "=" * 70)
    print("åˆ‡æ¢æ¨¡å‹åˆ° Qwen")
    print("=" * 70 + "\n")

    rag.switch_model(ModelProvider.QWEN)

    print("\nã€ä½¿ç”¨ Qwenã€‘")
    result = rag.query(question, top_k=3)
    rag.print_answer(result)


def test_custom_prompt():
    """æµ‹è¯•è‡ªå®šä¹‰ Prompt"""

    load_dotenv()

    print("\n" + "=" * 70)
    print("æµ‹è¯•ï¼šè‡ªå®šä¹‰ System Prompt")
    print("=" * 70 + "\n")

    # åˆå§‹åŒ–
    embedding = HuggingFaceEmbeddings(
        model_name="BAAI/bge-large-zh-v1.5",
        cache_folder=get_models_cache_dir(),
        model_kwargs={"device": "cpu", "local_files_only": True}
    )

    vectorstore = create_vectorstore_manager(
        embedding_model=embedding,
        persist_directory="../data/vectorstore"
    )

    rag = create_rag_pipeline(
        vectorstore_manager=vectorstore,
        model_provider=ModelProvider.DEEPSEEK,
        verbose=True
    )

    # è‡ªå®šä¹‰ System Prompt
    custom_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ³•å¾‹é¡¾é—®åŠ©æ‰‹ã€‚

åœ¨å›ç­”é—®é¢˜æ—¶ï¼š
1. ä½¿ç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€
2. é‡ç‚¹å…³æ³¨æ³•å¾‹æ¡æ¬¾å’Œæƒç›Š
3. å¦‚æœæ¶‰åŠé‡è¦æƒç›Šï¼Œç»™å‡ºæé†’
4. åŸºäºèµ„æ–™å®¢è§‚å›ç­”ï¼Œä¸åšè¿‡åº¦æ¨æ–­"""

    question = "åŠ³åŠ¨åˆåŒä¸­å¯¹åŠ³åŠ¨è€…çš„æƒç›Šä¿æŠ¤æœ‰å“ªäº›ï¼Ÿ"

    # æ£€ç´¢æ–‡æ¡£
    documents = rag.retrieve_documents(question, k=3)

    # ä½¿ç”¨è‡ªå®šä¹‰ prompt ç”Ÿæˆç­”æ¡ˆ
    answer = rag.generate_answer(question, documents, system_prompt=custom_prompt)

    print(f"\n{'=' * 70}")
    print(f"é—®é¢˜: {question}")
    print(f"{'=' * 70}\n")
    print(f"ğŸ“ ç­”æ¡ˆï¼š")
    print(answer)
    print(f"\n{'=' * 70}\n")


def main():
    """ä¸»å‡½æ•°"""
    try:
        # ä¸»æµ‹è¯•ï¼šå®Œæ•´ RAG æŸ¥è¯¢æµç¨‹
        test_rag_query()

        # å…¶ä»–æµ‹è¯•ï¼ˆå¯é€‰ï¼‰
        # test_model_switching()
        # test_custom_prompt()

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()